{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI4Finance-LLC/FinRL/blob/master/FinRL_ensemble_stock_trading_ICAIF_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXaoZs2lh1hi"
      },
      "source": [
        "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
        "\n",
        "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
        "\n",
        "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
        "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
        "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
        "* **Pytorch Version** \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGunVt8oLCVS"
      },
      "source": [
        "# Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOzAKQ-SLGX6"
      },
      "source": [
        "* [1. Problem Definition](#0)\n",
        "* [2. Getting Started - Load Python packages](#1)\n",
        "    * [2.1. Install Packages](#1.1)    \n",
        "    * [2.2. Check Additional Packages](#1.2)\n",
        "    * [2.3. Import Packages](#1.3)\n",
        "    * [2.4. Create Folders](#1.4)\n",
        "* [3. Download Data](#2)\n",
        "* [4. Preprocess Data](#3)        \n",
        "    * [4.1. Technical Indicators](#3.1)\n",
        "    * [4.2. Perform Feature Engineering](#3.2)\n",
        "* [5.Build Environment](#4)  \n",
        "    * [5.1. Training & Trade Data Split](#4.1)\n",
        "    * [5.2. User-defined Environment](#4.2)   \n",
        "    * [5.3. Initialize Environment](#4.3)    \n",
        "* [6.Implement DRL Algorithms](#5)  \n",
        "* [7.Backtesting Performance](#6)  \n",
        "    * [7.1. BackTestStats](#6.1)\n",
        "    * [7.2. BackTestPlot](#6.2)   \n",
        "    * [7.3. Baseline Stats](#6.3)   \n",
        "    * [7.3. Compare to Stock Market Index](#6.4)             "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sApkDlD9LIZv"
      },
      "source": [
        "<a id='0'></a>\n",
        "# Part 1. Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjLD2TZSLKZ-"
      },
      "source": [
        "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
        "\n",
        "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
        "\n",
        "\n",
        "* Action: The action space describes the allowed actions that the agent interacts with the\n",
        "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
        "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
        "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
        "values at state s′ and s, respectively\n",
        "\n",
        "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
        "our trading agent observes many different features to better learn in an interactive environment.\n",
        "\n",
        "* Environment: Dow 30 consituents\n",
        "\n",
        "\n",
        "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffsre789LY08"
      },
      "source": [
        "<a id='1'></a>\n",
        "# Part 2. Getting Started- Load Python Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy5_PTmOh1hj"
      },
      "source": [
        "<a id='1.1'></a>\n",
        "## 2.1. Install all the packages through FinRL library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPT0ipYE28wL",
        "outputId": "deed382f-2890-4a33-ec55-ed184ff656bb"
      },
      "outputs": [],
      "source": [
        "# ## install finrl library\n",
        "#!pip install git+https://github.com/AI4Finance-LLC/FinRL-Library.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osBHhVysOEzi"
      },
      "source": [
        "\n",
        "<a id='1.2'></a>\n",
        "## 2.2. Check if the additional packages needed are present, if not install them. \n",
        "* Yahoo Finance API\n",
        "* pandas\n",
        "* numpy\n",
        "* matplotlib\n",
        "* stockstats\n",
        "* OpenAI gym\n",
        "* stable-baselines\n",
        "* tensorflow\n",
        "* pyfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGv01K8Sh1hn"
      },
      "source": [
        "<a id='1.3'></a>\n",
        "## 2.3. Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EeMK7Uentj1V"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lPqeTTwoh1hn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# matplotlib.use('Agg')\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "%matplotlib inline\n",
        "from finrl.apps import config\n",
        "from finrl.neo_finrl.preprocessor.yahoodownloader import YahooDownloader\n",
        "from finrl.neo_finrl.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "from finrl.neo_finrl.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.drl_agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
        "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../FinRL-Library\")\n",
        "\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2owTj985RW4"
      },
      "source": [
        "<a id='1.4'></a>\n",
        "## 2.4. Create Folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w9A8CN5R5PuZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists(\"./\" + config.DATA_SAVE_DIR):\n",
        "    os.makedirs(\"./\" + config.DATA_SAVE_DIR)\n",
        "if not os.path.exists(\"./\" + config.TRAINED_MODEL_DIR):\n",
        "    os.makedirs(\"./\" + config.TRAINED_MODEL_DIR)\n",
        "if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n",
        "    os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)\n",
        "if not os.path.exists(\"./\" + config.RESULTS_DIR):\n",
        "    os.makedirs(\"./\" + config.RESULTS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A289rQWMh1hq"
      },
      "source": [
        "<a id='2'></a>\n",
        "# Part 3. Download Data\n",
        "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
        "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
        "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPeQ7iS-LoMm"
      },
      "source": [
        "\n",
        "\n",
        "-----\n",
        "class YahooDownloader:\n",
        "    Provides methods for retrieving daily stock data from\n",
        "    Yahoo Finance API\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        start_date : str\n",
        "            start date of the data (modified from config.py)\n",
        "        end_date : str\n",
        "            end date of the data (modified from config.py)\n",
        "        ticker_list : list\n",
        "            a list of stock tickers (modified from config.py)\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    fetch_data()\n",
        "        Fetches data from yahoo API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h3XJnvrbLp-C",
        "outputId": "68ab1da6-627f-476e-c471-1c655d54a4d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time_current =====2021-11-26_20:49\n"
          ]
        }
      ],
      "source": [
        "# from config.py start_date is a string\n",
        "config.START_DATE\n",
        "\n",
        "data_prefix = [f\"{config.DOW_30_TICKER}\", f\"{config.nas_choosen}\", f\"{config.sp_choosen}\"]\n",
        "path_mark = [\"dow\", \"nas\", \"sp\"]\n",
        "choose_number = 0\n",
        "time_current = datetime.now() + timedelta(hours=8)\n",
        "time_current = time_current.strftime(\"%Y-%m-%d_%H:%M\")\n",
        "print(f\"time_current ====={time_current}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzqRRTOX6aFu",
        "outputId": "524ed5d8-3b67-461f-ff9a-ae7494909e96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\n"
          ]
        }
      ],
      "source": [
        "print(config.DOW_30_TICKER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCKm4om-s9kE",
        "outputId": "cad51077-70f6-4e0d-acea-f90b8fc3e3ea"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "from pathlib import Path\n",
        "path = Path('datasets/dow_2021-09-24-12h19.csv')\n",
        "if path.is_file():\n",
        "    df = pd.read_csv(path)\n",
        "else:\n",
        "    df = YahooDownloader(start_date = '2009-01-01',\n",
        "                        end_date = '2021-07-06',\n",
        "                        ticker_list = config.DOW_30_TICKER).fetch_data()\n",
        "\"\"\"\n",
        "\n",
        "SAVE_PATH = f\"./datasets/{path_mark[choose_number]}.csv\"\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    df = pd.read_csv(SAVE_PATH)\n",
        "else:\n",
        "    #注意更改ticker_list\n",
        "    df = YahooDownloader(\n",
        "        config.START_DATE,  #'2000-01-01',\n",
        "        config.END_DATE,  # 2021-01-01，预计将改日期改为'2021-07-03'（今日日期）\n",
        "        ticker_list=config.DOW_30_TICKER#config.DOW_30_TICKER, config.nas_choosen, config.sp_choosen\n",
        "    ).fetch_data()\n",
        "    df.to_csv(SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "GiRuFOTOtj1Y",
        "outputId": "42241de4-9ab6-4944-a1fd-449672e3c286"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>date</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>tic</th>\n",
              "      <th>day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>0.936384</td>\n",
              "      <td>1.004464</td>\n",
              "      <td>0.907924</td>\n",
              "      <td>0.858137</td>\n",
              "      <td>535796800.0</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>47.995617</td>\n",
              "      <td>47.995617</td>\n",
              "      <td>45.515598</td>\n",
              "      <td>34.071674</td>\n",
              "      <td>6471267.0</td>\n",
              "      <td>AXP</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>41.437500</td>\n",
              "      <td>41.687500</td>\n",
              "      <td>39.812500</td>\n",
              "      <td>25.940277</td>\n",
              "      <td>2638200.0</td>\n",
              "      <td>BA</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>23.843750</td>\n",
              "      <td>24.500000</td>\n",
              "      <td>23.843750</td>\n",
              "      <td>13.620012</td>\n",
              "      <td>5055000.0</td>\n",
              "      <td>CAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>54.968750</td>\n",
              "      <td>55.125000</td>\n",
              "      <td>51.781250</td>\n",
              "      <td>39.840889</td>\n",
              "      <td>53076000.0</td>\n",
              "      <td>CSCO</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1        date       open       high  \\\n",
              "0           0             0               0  2000-01-03   0.936384   1.004464   \n",
              "1           1             1               1  2000-01-03  47.995617  47.995617   \n",
              "2           2             2               2  2000-01-03  41.437500  41.687500   \n",
              "3           3             3               3  2000-01-03  23.843750  24.500000   \n",
              "4           4             4               4  2000-01-03  54.968750  55.125000   \n",
              "\n",
              "         low      close       volume   tic  day  \n",
              "0   0.907924   0.858137  535796800.0  AAPL    0  \n",
              "1  45.515598  34.071674    6471267.0   AXP    0  \n",
              "2  39.812500  25.940277    2638200.0    BA    0  \n",
              "3  23.843750  13.620012    5055000.0   CAT    0  \n",
              "4  51.781250  39.840889   53076000.0  CSCO    0  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DSw4ZEzVtj1Z",
        "outputId": "470c69a4-b526-41eb-9e35-e325793b7529"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>date</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>tic</th>\n",
              "      <th>day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>161462</th>\n",
              "      <td>161462</td>\n",
              "      <td>161462</td>\n",
              "      <td>161462</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>230.910004</td>\n",
              "      <td>231.839996</td>\n",
              "      <td>228.660004</td>\n",
              "      <td>229.100006</td>\n",
              "      <td>9405100.0</td>\n",
              "      <td>V</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161463</th>\n",
              "      <td>161463</td>\n",
              "      <td>161463</td>\n",
              "      <td>161463</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>54.709999</td>\n",
              "      <td>55.139999</td>\n",
              "      <td>54.580002</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>18405600.0</td>\n",
              "      <td>VZ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161464</th>\n",
              "      <td>161464</td>\n",
              "      <td>161464</td>\n",
              "      <td>161464</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>48.459999</td>\n",
              "      <td>50.869999</td>\n",
              "      <td>48.389999</td>\n",
              "      <td>50.750000</td>\n",
              "      <td>11814400.0</td>\n",
              "      <td>WBA</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161465</th>\n",
              "      <td>161465</td>\n",
              "      <td>161465</td>\n",
              "      <td>161465</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>147.990005</td>\n",
              "      <td>148.440002</td>\n",
              "      <td>147.240005</td>\n",
              "      <td>148.100006</td>\n",
              "      <td>8359500.0</td>\n",
              "      <td>WMT</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161466</th>\n",
              "      <td>161466</td>\n",
              "      <td>161466</td>\n",
              "      <td>161466</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>54.930000</td>\n",
              "      <td>55.310001</td>\n",
              "      <td>54.459999</td>\n",
              "      <td>54.520000</td>\n",
              "      <td>27104100.0</td>\n",
              "      <td>XOM</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1        date        open  \\\n",
              "161462      161462        161462          161462  2021-08-31  230.910004   \n",
              "161463      161463        161463          161463  2021-08-31   54.709999   \n",
              "161464      161464        161464          161464  2021-08-31   48.459999   \n",
              "161465      161465        161465          161465  2021-08-31  147.990005   \n",
              "161466      161466        161466          161466  2021-08-31   54.930000   \n",
              "\n",
              "              high         low       close      volume  tic  day  \n",
              "161462  231.839996  228.660004  229.100006   9405100.0    V    1  \n",
              "161463   55.139999   54.580002   55.000000  18405600.0   VZ    1  \n",
              "161464   50.869999   48.389999   50.750000  11814400.0  WBA    1  \n",
              "161465  148.440002  147.240005  148.100006   8359500.0  WMT    1  \n",
              "161466   55.310001   54.459999   54.520000  27104100.0  XOM    1  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV3HrZHLh1hy",
        "outputId": "90359d3b-bdcf-4807-87de-843ff729aaa2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(161467, 11)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "4hYkeaPiICHS",
        "outputId": "7606cbff-735a-4e34-8059-d3a2f2635921"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>date</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>tic</th>\n",
              "      <th>day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>0.936384</td>\n",
              "      <td>1.004464</td>\n",
              "      <td>0.907924</td>\n",
              "      <td>0.858137</td>\n",
              "      <td>535796800.0</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>47.995617</td>\n",
              "      <td>47.995617</td>\n",
              "      <td>45.515598</td>\n",
              "      <td>34.071674</td>\n",
              "      <td>6471267.0</td>\n",
              "      <td>AXP</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>41.437500</td>\n",
              "      <td>41.687500</td>\n",
              "      <td>39.812500</td>\n",
              "      <td>25.940277</td>\n",
              "      <td>2638200.0</td>\n",
              "      <td>BA</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>23.843750</td>\n",
              "      <td>24.500000</td>\n",
              "      <td>23.843750</td>\n",
              "      <td>13.620012</td>\n",
              "      <td>5055000.0</td>\n",
              "      <td>CAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>54.968750</td>\n",
              "      <td>55.125000</td>\n",
              "      <td>51.781250</td>\n",
              "      <td>39.840889</td>\n",
              "      <td>53076000.0</td>\n",
              "      <td>CSCO</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1        date       open       high  \\\n",
              "0           0             0               0  2000-01-03   0.936384   1.004464   \n",
              "1           1             1               1  2000-01-03  47.995617  47.995617   \n",
              "2           2             2               2  2000-01-03  41.437500  41.687500   \n",
              "3           3             3               3  2000-01-03  23.843750  24.500000   \n",
              "4           4             4               4  2000-01-03  54.968750  55.125000   \n",
              "\n",
              "         low      close       volume   tic  day  \n",
              "0   0.907924   0.858137  535796800.0  AAPL    0  \n",
              "1  45.515598  34.071674    6471267.0   AXP    0  \n",
              "2  39.812500  25.940277    2638200.0    BA    0  \n",
              "3  23.843750  13.620012    5055000.0   CAT    0  \n",
              "4  51.781250  39.840889   53076000.0  CSCO    0  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sort_values(['date','tic']).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2vryMsdNL9H",
        "outputId": "663be644-2580-47d7-b535-8edac5088744"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df.tic.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcNyXa7RNPrF",
        "outputId": "7e7d1953-b3e3-4486-ccf4-86c80372b9a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AAPL    5451\n",
              "AXP     5451\n",
              "XOM     5451\n",
              "WMT     5451\n",
              "WBA     5451\n",
              "VZ      5451\n",
              "UNH     5451\n",
              "TRV     5451\n",
              "RTX     5451\n",
              "PG      5451\n",
              "PFE     5451\n",
              "NKE     5451\n",
              "MSFT    5451\n",
              "MRK     5451\n",
              "MMM     5451\n",
              "MCD     5451\n",
              "KO      5451\n",
              "JPM     5451\n",
              "JNJ     5451\n",
              "INTC    5451\n",
              "IBM     5451\n",
              "HD      5451\n",
              "GS      5451\n",
              "DIS     5451\n",
              "DD      5451\n",
              "CVX     5451\n",
              "CSCO    5451\n",
              "CAT     5451\n",
              "BA      5451\n",
              "V       3388\n",
              "Name: tic, dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.tic.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqC6c40Zh1iH"
      },
      "source": [
        "# Part 4: Preprocess Data\n",
        "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
        "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
        "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kM5bH9uroCeg"
      },
      "outputs": [],
      "source": [
        "tech_indicators = ['macd',\n",
        " 'rsi_30',\n",
        " 'cci_30',\n",
        " 'dx_30']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgXfBcjxtj1a",
        "outputId": "1e5c4398-302f-40d9-95eb-08eb91e81f35",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "Successfully added turbulence index\n"
          ]
        }
      ],
      "source": [
        "fe = FeatureEngineer(\n",
        "                    use_technical_indicator=True,\n",
        "                    tech_indicator_list = tech_indicators,\n",
        "                    use_turbulence=True,\n",
        "                    user_defined_feature = False)\n",
        "\n",
        "csv_name_func_processed = lambda time: f\"./datasets/ensemble_{path_mark[choose_number]}_{time}_processed.csv\"\n",
        "\n",
        "SAVE_PATH = csv_name_func_processed(time_current)\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    processed = pd.read_csv(SAVE_PATH)\n",
        "else:\n",
        "    processed = fe.preprocess_data(df)\n",
        "    processed.to_csv(SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "grvhGJJII3Xn",
        "outputId": "dceee58f-faaf-44c8-c516-2ab5b58f506d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>date</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>tic</th>\n",
              "      <th>day</th>\n",
              "      <th>macd</th>\n",
              "      <th>rsi_30</th>\n",
              "      <th>cci_30</th>\n",
              "      <th>dx_30</th>\n",
              "      <th>turbulence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>158049</th>\n",
              "      <td>161436</td>\n",
              "      <td>161436</td>\n",
              "      <td>161436</td>\n",
              "      <td>2021-08-30</td>\n",
              "      <td>56.240002</td>\n",
              "      <td>56.340000</td>\n",
              "      <td>55.119999</td>\n",
              "      <td>55.160000</td>\n",
              "      <td>14086100.0</td>\n",
              "      <td>XOM</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.746931</td>\n",
              "      <td>45.930346</td>\n",
              "      <td>-48.680407</td>\n",
              "      <td>12.283893</td>\n",
              "      <td>14.647959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158050</th>\n",
              "      <td>161437</td>\n",
              "      <td>161437</td>\n",
              "      <td>161437</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>152.660004</td>\n",
              "      <td>152.800003</td>\n",
              "      <td>151.289993</td>\n",
              "      <td>151.830002</td>\n",
              "      <td>86453100.0</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>1</td>\n",
              "      <td>1.715120</td>\n",
              "      <td>60.749028</td>\n",
              "      <td>194.782125</td>\n",
              "      <td>17.767376</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158051</th>\n",
              "      <td>161438</td>\n",
              "      <td>161438</td>\n",
              "      <td>161438</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>164.100006</td>\n",
              "      <td>166.429993</td>\n",
              "      <td>163.720001</td>\n",
              "      <td>165.960007</td>\n",
              "      <td>3536300.0</td>\n",
              "      <td>AXP</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.972211</td>\n",
              "      <td>50.949790</td>\n",
              "      <td>-45.813668</td>\n",
              "      <td>0.684613</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158052</th>\n",
              "      <td>161439</td>\n",
              "      <td>161439</td>\n",
              "      <td>161439</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>217.330002</td>\n",
              "      <td>221.330002</td>\n",
              "      <td>217.089996</td>\n",
              "      <td>219.500000</td>\n",
              "      <td>6764500.0</td>\n",
              "      <td>BA</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.132416</td>\n",
              "      <td>45.740691</td>\n",
              "      <td>-69.975230</td>\n",
              "      <td>23.128916</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158053</th>\n",
              "      <td>161440</td>\n",
              "      <td>161440</td>\n",
              "      <td>161440</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>211.279999</td>\n",
              "      <td>212.300003</td>\n",
              "      <td>210.419998</td>\n",
              "      <td>210.869995</td>\n",
              "      <td>2857900.0</td>\n",
              "      <td>CAT</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.271620</td>\n",
              "      <td>47.704373</td>\n",
              "      <td>5.337284</td>\n",
              "      <td>4.277340</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158054</th>\n",
              "      <td>161441</td>\n",
              "      <td>161441</td>\n",
              "      <td>161441</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>59.110001</td>\n",
              "      <td>59.180000</td>\n",
              "      <td>58.580002</td>\n",
              "      <td>59.020000</td>\n",
              "      <td>19873900.0</td>\n",
              "      <td>CSCO</td>\n",
              "      <td>1</td>\n",
              "      <td>1.202027</td>\n",
              "      <td>65.947613</td>\n",
              "      <td>126.383954</td>\n",
              "      <td>37.809397</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158055</th>\n",
              "      <td>161442</td>\n",
              "      <td>161442</td>\n",
              "      <td>161442</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>98.139999</td>\n",
              "      <td>98.410004</td>\n",
              "      <td>96.639999</td>\n",
              "      <td>96.769997</td>\n",
              "      <td>15676900.0</td>\n",
              "      <td>CVX</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.804458</td>\n",
              "      <td>45.373636</td>\n",
              "      <td>-80.993548</td>\n",
              "      <td>19.363579</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158056</th>\n",
              "      <td>161443</td>\n",
              "      <td>161443</td>\n",
              "      <td>161443</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>73.839996</td>\n",
              "      <td>74.190002</td>\n",
              "      <td>73.309998</td>\n",
              "      <td>74.019997</td>\n",
              "      <td>2948300.0</td>\n",
              "      <td>DD</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.516666</td>\n",
              "      <td>45.460026</td>\n",
              "      <td>-79.434879</td>\n",
              "      <td>17.407022</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158057</th>\n",
              "      <td>161444</td>\n",
              "      <td>161444</td>\n",
              "      <td>161444</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>179.250000</td>\n",
              "      <td>182.979996</td>\n",
              "      <td>178.529999</td>\n",
              "      <td>181.300003</td>\n",
              "      <td>8620000.0</td>\n",
              "      <td>DIS</td>\n",
              "      <td>1</td>\n",
              "      <td>0.655867</td>\n",
              "      <td>53.753650</td>\n",
              "      <td>142.697023</td>\n",
              "      <td>22.860571</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158058</th>\n",
              "      <td>161445</td>\n",
              "      <td>161445</td>\n",
              "      <td>161445</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>412.109985</td>\n",
              "      <td>414.859985</td>\n",
              "      <td>408.760010</td>\n",
              "      <td>413.510010</td>\n",
              "      <td>1958100.0</td>\n",
              "      <td>GS</td>\n",
              "      <td>1</td>\n",
              "      <td>8.698176</td>\n",
              "      <td>61.402192</td>\n",
              "      <td>83.776940</td>\n",
              "      <td>30.736524</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158059</th>\n",
              "      <td>161446</td>\n",
              "      <td>161446</td>\n",
              "      <td>161446</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>327.000000</td>\n",
              "      <td>328.269989</td>\n",
              "      <td>323.619995</td>\n",
              "      <td>324.529999</td>\n",
              "      <td>4183100.0</td>\n",
              "      <td>HD</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.210802</td>\n",
              "      <td>52.078993</td>\n",
              "      <td>-45.877215</td>\n",
              "      <td>1.096906</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158060</th>\n",
              "      <td>161447</td>\n",
              "      <td>161447</td>\n",
              "      <td>161447</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>139.539993</td>\n",
              "      <td>140.940002</td>\n",
              "      <td>138.949997</td>\n",
              "      <td>140.339996</td>\n",
              "      <td>4235100.0</td>\n",
              "      <td>IBM</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.419160</td>\n",
              "      <td>50.029628</td>\n",
              "      <td>-52.866090</td>\n",
              "      <td>4.411810</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158061</th>\n",
              "      <td>161448</td>\n",
              "      <td>161448</td>\n",
              "      <td>161448</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>54.439999</td>\n",
              "      <td>54.500000</td>\n",
              "      <td>53.660000</td>\n",
              "      <td>54.060001</td>\n",
              "      <td>22350100.0</td>\n",
              "      <td>INTC</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.168787</td>\n",
              "      <td>48.113028</td>\n",
              "      <td>54.421808</td>\n",
              "      <td>0.419660</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158062</th>\n",
              "      <td>161449</td>\n",
              "      <td>161449</td>\n",
              "      <td>161449</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>173.490005</td>\n",
              "      <td>174.490005</td>\n",
              "      <td>172.479996</td>\n",
              "      <td>173.130005</td>\n",
              "      <td>7106800.0</td>\n",
              "      <td>JNJ</td>\n",
              "      <td>1</td>\n",
              "      <td>0.932500</td>\n",
              "      <td>54.474404</td>\n",
              "      <td>-15.285250</td>\n",
              "      <td>10.052281</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158063</th>\n",
              "      <td>161450</td>\n",
              "      <td>161450</td>\n",
              "      <td>161450</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>160.440002</td>\n",
              "      <td>161.369995</td>\n",
              "      <td>159.350006</td>\n",
              "      <td>159.949997</td>\n",
              "      <td>12340800.0</td>\n",
              "      <td>JPM</td>\n",
              "      <td>1</td>\n",
              "      <td>1.639263</td>\n",
              "      <td>54.341243</td>\n",
              "      <td>82.167488</td>\n",
              "      <td>16.905366</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158064</th>\n",
              "      <td>161451</td>\n",
              "      <td>161451</td>\n",
              "      <td>161451</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>56.160000</td>\n",
              "      <td>56.520000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>56.310001</td>\n",
              "      <td>14185700.0</td>\n",
              "      <td>KO</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.042161</td>\n",
              "      <td>53.105540</td>\n",
              "      <td>-73.692155</td>\n",
              "      <td>3.500160</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158065</th>\n",
              "      <td>161452</td>\n",
              "      <td>161452</td>\n",
              "      <td>161452</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>235.750000</td>\n",
              "      <td>237.729996</td>\n",
              "      <td>234.669998</td>\n",
              "      <td>237.460007</td>\n",
              "      <td>2938100.0</td>\n",
              "      <td>MCD</td>\n",
              "      <td>1</td>\n",
              "      <td>0.223354</td>\n",
              "      <td>52.918204</td>\n",
              "      <td>-43.370740</td>\n",
              "      <td>17.295128</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158066</th>\n",
              "      <td>161453</td>\n",
              "      <td>161453</td>\n",
              "      <td>161453</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>195.850006</td>\n",
              "      <td>195.990005</td>\n",
              "      <td>194.149994</td>\n",
              "      <td>194.740005</td>\n",
              "      <td>2813900.0</td>\n",
              "      <td>MMM</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.745519</td>\n",
              "      <td>47.711566</td>\n",
              "      <td>-100.406518</td>\n",
              "      <td>7.352552</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158067</th>\n",
              "      <td>161454</td>\n",
              "      <td>161454</td>\n",
              "      <td>161454</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>76.790001</td>\n",
              "      <td>77.099998</td>\n",
              "      <td>76.029999</td>\n",
              "      <td>76.290001</td>\n",
              "      <td>10949500.0</td>\n",
              "      <td>MRK</td>\n",
              "      <td>1</td>\n",
              "      <td>0.014081</td>\n",
              "      <td>49.544289</td>\n",
              "      <td>-30.650520</td>\n",
              "      <td>0.784162</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158068</th>\n",
              "      <td>161455</td>\n",
              "      <td>161455</td>\n",
              "      <td>161455</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>304.420013</td>\n",
              "      <td>304.500000</td>\n",
              "      <td>301.500000</td>\n",
              "      <td>301.880005</td>\n",
              "      <td>26285300.0</td>\n",
              "      <td>MSFT</td>\n",
              "      <td>1</td>\n",
              "      <td>5.512404</td>\n",
              "      <td>65.793586</td>\n",
              "      <td>123.647191</td>\n",
              "      <td>42.661767</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158069</th>\n",
              "      <td>161456</td>\n",
              "      <td>161456</td>\n",
              "      <td>161456</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>167.550003</td>\n",
              "      <td>167.679993</td>\n",
              "      <td>164.199997</td>\n",
              "      <td>164.740005</td>\n",
              "      <td>7545600.0</td>\n",
              "      <td>NKE</td>\n",
              "      <td>1</td>\n",
              "      <td>1.086216</td>\n",
              "      <td>55.659213</td>\n",
              "      <td>-71.140529</td>\n",
              "      <td>12.027498</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158070</th>\n",
              "      <td>161457</td>\n",
              "      <td>161457</td>\n",
              "      <td>161457</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>46.720001</td>\n",
              "      <td>47.139999</td>\n",
              "      <td>45.930000</td>\n",
              "      <td>46.070000</td>\n",
              "      <td>34547800.0</td>\n",
              "      <td>PFE</td>\n",
              "      <td>1</td>\n",
              "      <td>1.081450</td>\n",
              "      <td>56.948839</td>\n",
              "      <td>14.013299</td>\n",
              "      <td>21.107411</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158071</th>\n",
              "      <td>161458</td>\n",
              "      <td>161458</td>\n",
              "      <td>161458</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>143.039993</td>\n",
              "      <td>143.529999</td>\n",
              "      <td>142.050003</td>\n",
              "      <td>142.389999</td>\n",
              "      <td>9642400.0</td>\n",
              "      <td>PG</td>\n",
              "      <td>1</td>\n",
              "      <td>0.809085</td>\n",
              "      <td>55.817872</td>\n",
              "      <td>19.967485</td>\n",
              "      <td>14.157921</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158072</th>\n",
              "      <td>161459</td>\n",
              "      <td>161459</td>\n",
              "      <td>161459</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>84.650002</td>\n",
              "      <td>85.419998</td>\n",
              "      <td>84.339996</td>\n",
              "      <td>84.760002</td>\n",
              "      <td>3969600.0</td>\n",
              "      <td>RTX</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.297136</td>\n",
              "      <td>49.116676</td>\n",
              "      <td>-97.369368</td>\n",
              "      <td>13.887225</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158073</th>\n",
              "      <td>161460</td>\n",
              "      <td>161460</td>\n",
              "      <td>161460</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>158.850006</td>\n",
              "      <td>160.800003</td>\n",
              "      <td>158.710007</td>\n",
              "      <td>159.710007</td>\n",
              "      <td>1568700.0</td>\n",
              "      <td>TRV</td>\n",
              "      <td>1</td>\n",
              "      <td>2.314197</td>\n",
              "      <td>56.038929</td>\n",
              "      <td>73.947534</td>\n",
              "      <td>27.768804</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158074</th>\n",
              "      <td>161461</td>\n",
              "      <td>161461</td>\n",
              "      <td>161461</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>417.000000</td>\n",
              "      <td>420.739990</td>\n",
              "      <td>414.220001</td>\n",
              "      <td>416.269989</td>\n",
              "      <td>4249100.0</td>\n",
              "      <td>UNH</td>\n",
              "      <td>1</td>\n",
              "      <td>1.801732</td>\n",
              "      <td>52.549568</td>\n",
              "      <td>11.099638</td>\n",
              "      <td>9.546535</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158075</th>\n",
              "      <td>161463</td>\n",
              "      <td>161463</td>\n",
              "      <td>161463</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>54.709999</td>\n",
              "      <td>55.139999</td>\n",
              "      <td>54.580002</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>18405600.0</td>\n",
              "      <td>VZ</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.253341</td>\n",
              "      <td>44.653849</td>\n",
              "      <td>-120.624403</td>\n",
              "      <td>24.242250</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158076</th>\n",
              "      <td>161464</td>\n",
              "      <td>161464</td>\n",
              "      <td>161464</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>48.459999</td>\n",
              "      <td>50.869999</td>\n",
              "      <td>48.389999</td>\n",
              "      <td>50.750000</td>\n",
              "      <td>11814400.0</td>\n",
              "      <td>WBA</td>\n",
              "      <td>1</td>\n",
              "      <td>0.388522</td>\n",
              "      <td>56.376587</td>\n",
              "      <td>155.229924</td>\n",
              "      <td>20.132924</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158077</th>\n",
              "      <td>161465</td>\n",
              "      <td>161465</td>\n",
              "      <td>161465</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>147.990005</td>\n",
              "      <td>148.440002</td>\n",
              "      <td>147.240005</td>\n",
              "      <td>148.100006</td>\n",
              "      <td>8359500.0</td>\n",
              "      <td>WMT</td>\n",
              "      <td>1</td>\n",
              "      <td>1.379987</td>\n",
              "      <td>57.798913</td>\n",
              "      <td>35.525528</td>\n",
              "      <td>14.285037</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158078</th>\n",
              "      <td>161466</td>\n",
              "      <td>161466</td>\n",
              "      <td>161466</td>\n",
              "      <td>2021-08-31</td>\n",
              "      <td>54.930000</td>\n",
              "      <td>55.310001</td>\n",
              "      <td>54.459999</td>\n",
              "      <td>54.520000</td>\n",
              "      <td>27104100.0</td>\n",
              "      <td>XOM</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.751227</td>\n",
              "      <td>44.669736</td>\n",
              "      <td>-88.053329</td>\n",
              "      <td>17.850604</td>\n",
              "      <td>20.744256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1        date        open  \\\n",
              "158049      161436        161436          161436  2021-08-30   56.240002   \n",
              "158050      161437        161437          161437  2021-08-31  152.660004   \n",
              "158051      161438        161438          161438  2021-08-31  164.100006   \n",
              "158052      161439        161439          161439  2021-08-31  217.330002   \n",
              "158053      161440        161440          161440  2021-08-31  211.279999   \n",
              "158054      161441        161441          161441  2021-08-31   59.110001   \n",
              "158055      161442        161442          161442  2021-08-31   98.139999   \n",
              "158056      161443        161443          161443  2021-08-31   73.839996   \n",
              "158057      161444        161444          161444  2021-08-31  179.250000   \n",
              "158058      161445        161445          161445  2021-08-31  412.109985   \n",
              "158059      161446        161446          161446  2021-08-31  327.000000   \n",
              "158060      161447        161447          161447  2021-08-31  139.539993   \n",
              "158061      161448        161448          161448  2021-08-31   54.439999   \n",
              "158062      161449        161449          161449  2021-08-31  173.490005   \n",
              "158063      161450        161450          161450  2021-08-31  160.440002   \n",
              "158064      161451        161451          161451  2021-08-31   56.160000   \n",
              "158065      161452        161452          161452  2021-08-31  235.750000   \n",
              "158066      161453        161453          161453  2021-08-31  195.850006   \n",
              "158067      161454        161454          161454  2021-08-31   76.790001   \n",
              "158068      161455        161455          161455  2021-08-31  304.420013   \n",
              "158069      161456        161456          161456  2021-08-31  167.550003   \n",
              "158070      161457        161457          161457  2021-08-31   46.720001   \n",
              "158071      161458        161458          161458  2021-08-31  143.039993   \n",
              "158072      161459        161459          161459  2021-08-31   84.650002   \n",
              "158073      161460        161460          161460  2021-08-31  158.850006   \n",
              "158074      161461        161461          161461  2021-08-31  417.000000   \n",
              "158075      161463        161463          161463  2021-08-31   54.709999   \n",
              "158076      161464        161464          161464  2021-08-31   48.459999   \n",
              "158077      161465        161465          161465  2021-08-31  147.990005   \n",
              "158078      161466        161466          161466  2021-08-31   54.930000   \n",
              "\n",
              "              high         low       close      volume   tic  day      macd  \\\n",
              "158049   56.340000   55.119999   55.160000  14086100.0   XOM    0 -0.746931   \n",
              "158050  152.800003  151.289993  151.830002  86453100.0  AAPL    1  1.715120   \n",
              "158051  166.429993  163.720001  165.960007   3536300.0   AXP    1 -0.972211   \n",
              "158052  221.330002  217.089996  219.500000   6764500.0    BA    1 -3.132416   \n",
              "158053  212.300003  210.419998  210.869995   2857900.0   CAT    1 -0.271620   \n",
              "158054   59.180000   58.580002   59.020000  19873900.0  CSCO    1  1.202027   \n",
              "158055   98.410004   96.639999   96.769997  15676900.0   CVX    1 -0.804458   \n",
              "158056   74.190002   73.309998   74.019997   2948300.0    DD    1 -0.516666   \n",
              "158057  182.979996  178.529999  181.300003   8620000.0   DIS    1  0.655867   \n",
              "158058  414.859985  408.760010  413.510010   1958100.0    GS    1  8.698176   \n",
              "158059  328.269989  323.619995  324.529999   4183100.0    HD    1 -0.210802   \n",
              "158060  140.940002  138.949997  140.339996   4235100.0   IBM    1 -0.419160   \n",
              "158061   54.500000   53.660000   54.060001  22350100.0  INTC    1 -0.168787   \n",
              "158062  174.490005  172.479996  173.130005   7106800.0   JNJ    1  0.932500   \n",
              "158063  161.369995  159.350006  159.949997  12340800.0   JPM    1  1.639263   \n",
              "158064   56.520000   56.000000   56.310001  14185700.0    KO    1 -0.042161   \n",
              "158065  237.729996  234.669998  237.460007   2938100.0   MCD    1  0.223354   \n",
              "158066  195.990005  194.149994  194.740005   2813900.0   MMM    1 -0.745519   \n",
              "158067   77.099998   76.029999   76.290001  10949500.0   MRK    1  0.014081   \n",
              "158068  304.500000  301.500000  301.880005  26285300.0  MSFT    1  5.512404   \n",
              "158069  167.679993  164.199997  164.740005   7545600.0   NKE    1  1.086216   \n",
              "158070   47.139999   45.930000   46.070000  34547800.0   PFE    1  1.081450   \n",
              "158071  143.529999  142.050003  142.389999   9642400.0    PG    1  0.809085   \n",
              "158072   85.419998   84.339996   84.760002   3969600.0   RTX    1 -0.297136   \n",
              "158073  160.800003  158.710007  159.710007   1568700.0   TRV    1  2.314197   \n",
              "158074  420.739990  414.220001  416.269989   4249100.0   UNH    1  1.801732   \n",
              "158075   55.139999   54.580002   55.000000  18405600.0    VZ    1 -0.253341   \n",
              "158076   50.869999   48.389999   50.750000  11814400.0   WBA    1  0.388522   \n",
              "158077  148.440002  147.240005  148.100006   8359500.0   WMT    1  1.379987   \n",
              "158078   55.310001   54.459999   54.520000  27104100.0   XOM    1 -0.751227   \n",
              "\n",
              "           rsi_30      cci_30      dx_30  turbulence  \n",
              "158049  45.930346  -48.680407  12.283893   14.647959  \n",
              "158050  60.749028  194.782125  17.767376   20.744256  \n",
              "158051  50.949790  -45.813668   0.684613   20.744256  \n",
              "158052  45.740691  -69.975230  23.128916   20.744256  \n",
              "158053  47.704373    5.337284   4.277340   20.744256  \n",
              "158054  65.947613  126.383954  37.809397   20.744256  \n",
              "158055  45.373636  -80.993548  19.363579   20.744256  \n",
              "158056  45.460026  -79.434879  17.407022   20.744256  \n",
              "158057  53.753650  142.697023  22.860571   20.744256  \n",
              "158058  61.402192   83.776940  30.736524   20.744256  \n",
              "158059  52.078993  -45.877215   1.096906   20.744256  \n",
              "158060  50.029628  -52.866090   4.411810   20.744256  \n",
              "158061  48.113028   54.421808   0.419660   20.744256  \n",
              "158062  54.474404  -15.285250  10.052281   20.744256  \n",
              "158063  54.341243   82.167488  16.905366   20.744256  \n",
              "158064  53.105540  -73.692155   3.500160   20.744256  \n",
              "158065  52.918204  -43.370740  17.295128   20.744256  \n",
              "158066  47.711566 -100.406518   7.352552   20.744256  \n",
              "158067  49.544289  -30.650520   0.784162   20.744256  \n",
              "158068  65.793586  123.647191  42.661767   20.744256  \n",
              "158069  55.659213  -71.140529  12.027498   20.744256  \n",
              "158070  56.948839   14.013299  21.107411   20.744256  \n",
              "158071  55.817872   19.967485  14.157921   20.744256  \n",
              "158072  49.116676  -97.369368  13.887225   20.744256  \n",
              "158073  56.038929   73.947534  27.768804   20.744256  \n",
              "158074  52.549568   11.099638   9.546535   20.744256  \n",
              "158075  44.653849 -120.624403  24.242250   20.744256  \n",
              "158076  56.376587  155.229924  20.132924   20.744256  \n",
              "158077  57.798913   35.525528  14.285037   20.744256  \n",
              "158078  44.669736  -88.053329  17.850604   20.744256  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed.tail(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QsYaY0Dh1iw"
      },
      "source": [
        "<a id='4'></a>\n",
        "# Part 5. Design Environment\n",
        "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
        "\n",
        "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
        "\n",
        "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2zqII8rMIqn",
        "outputId": "5ac63d59-cf39-49a2-cfe7-7fa859a9c96b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stock Dimension: 29, State Space: 175\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(processed.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(tech_indicators)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AWyp84Ltto19"
      },
      "outputs": [],
      "source": [
        "env_kwargs = {\n",
        "    \"hmax\": 100, \n",
        "    \"initial_amount\": 1000000, \n",
        "    \"buy_cost_pct\": 0.001, \n",
        "    \"sell_cost_pct\": 0.001, \n",
        "    \"state_space\": state_space, \n",
        "    \"stock_dim\": stock_dimension, \n",
        "    \"tech_indicator_list\": tech_indicators,\n",
        "    \"action_space\": stock_dimension, \n",
        "    \"reward_scaling\": 1e-4,\n",
        "    \"print_verbosity\":5\n",
        "    \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "<a id='5'></a>\n",
        "# Part 6: Implement DRL Algorithms\n",
        "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
        "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
        "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
        "design their own DRL algorithms by adapting these DRL algorithms.\n",
        "\n",
        "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "v-gthCxMtj1d"
      },
      "outputs": [],
      "source": [
        "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
        "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
        "\"\"\"\n",
        "train_start = '2009-01-01'\n",
        "train_end = '2020-04-01'\n",
        "val_test_start = '2020-04-01'\n",
        "val_test_end = '2021-07-20'\n",
        "\"\"\"\n",
        "train_start = \"2009-01-01\"\n",
        "train_end = \"2015-12-31\"\n",
        "val_test_start = \"2015-12-31\"\n",
        "val_test_end = \"2021-05-03\"\n",
        "\n",
        "ensemble_agent = DRLEnsembleAgent(df=processed,\n",
        "                 train_period=(train_start,train_end),\n",
        "                 val_test_period=(val_test_start,val_test_end),\n",
        "                 rebalance_window=rebalance_window, \n",
        "                 validation_window=validation_window, \n",
        "                 **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KsfEHa_Etj1d",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "A2C_model_kwargs = {\n",
        "                    'n_steps': 5,\n",
        "                    'ent_coef': 0.01,\n",
        "                    'learning_rate': 0.0005\n",
        "                    }\n",
        "\n",
        "PPO_model_kwargs = {\n",
        "                    \"ent_coef\":0.01,\n",
        "                    \"n_steps\": 2048,\n",
        "                    \"learning_rate\": 0.00025,\n",
        "                    \"batch_size\": 64\n",
        "                    }\n",
        "\n",
        "DDPG_model_kwargs = {\n",
        "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
        "                      \"buffer_size\": 100_000,\n",
        "                      \"learning_rate\": 0.000005,\n",
        "                      \"batch_size\": 64\n",
        "                    }\n",
        "\n",
        "timesteps_dict = {'a2c' : 30_000, \n",
        "                 'ppo' : 100_000, \n",
        "                 'ddpg' : 10_000\n",
        "                 }\n",
        "\n",
        "\n",
        "timesteps_dict = {'a2c' : 10_000, \n",
        "                 'ppo' : 10_000, \n",
        "                 'ddpg' : 10_000\n",
        "                 }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1lyCECstj1e",
        "outputId": "b82aa928-8212-4238-f176-b9f15bfd73e8",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  167.99271650842078\n",
            "======Model training from:  2009-01-01 to  2016-01-04\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0005}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_126_54\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 52          |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0.547       |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | 23.9        |\n",
            "|    reward             | -0.12799367 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.351       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 51         |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.18      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 6.61       |\n",
            "|    reward             | -1.6286542 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.336      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 54        |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.00125  |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -133      |\n",
            "|    reward             | 2.4559488 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 16.4      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 57          |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 34          |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | -0.0999     |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | 14.7        |\n",
            "|    reward             | -0.24808843 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.9         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 58        |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 42        |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 2.32      |\n",
            "|    reward             | 1.1182772 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.37      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 56         |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 52         |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0.273      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 97.6       |\n",
            "|    reward             | -1.2883741 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 5.99       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 56         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 61         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.09      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 1.91       |\n",
            "|    reward             | 0.06547223 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 7.83       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 56         |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 70         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.00955   |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -101       |\n",
            "|    reward             | 0.48476252 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 6.89       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 56         |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 79         |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.0735    |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -209       |\n",
            "|    reward             | 0.31914523 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 32.8       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 55         |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 89         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 201        |\n",
            "|    reward             | 0.21845192 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 27.8       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 55          |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 99          |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.3       |\n",
            "|    explained_variance | -0.145      |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | -129        |\n",
            "|    reward             | 0.052486755 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 12.3        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 54         |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 110        |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.0632    |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 53.9       |\n",
            "|    reward             | -3.2852254 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 5.26       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 54        |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 119       |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -0.0584   |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 70.6      |\n",
            "|    reward             | 1.1366884 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 5.29      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 55        |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 126       |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -0.173    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -57.9     |\n",
            "|    reward             | 1.5750879 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 12.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 55         |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 134        |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | -0.0101    |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 55.5       |\n",
            "|    reward             | -0.0841245 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.91       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 56        |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 141       |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -0.0749   |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 98.6      |\n",
            "|    reward             | 0.4958351 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 6.69      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 56        |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 150       |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -48.7     |\n",
            "|    reward             | -3.647977 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.27      |\n",
            "-------------------------------------\n",
            "day: 1761, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2279424.23\n",
            "total_reward: 1279424.23\n",
            "total_cost: 59898.22\n",
            "total_trades: 37606\n",
            "Sharpe: 0.837\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 56        |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 158       |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0.0865    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -36       |\n",
            "|    reward             | 2.2568145 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 3.3       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 56        |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 167       |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -238      |\n",
            "|    reward             | -4.129067 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 35        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 56         |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 176        |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 96.2       |\n",
            "|    reward             | -1.3131542 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 5.42       |\n",
            "--------------------------------------\n",
            "======A2C Validation from:  2016-01-04 to  2016-04-05\n",
            "A2C Sharpe Ratio:  0.06509322904000167\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_126_54\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 74          |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 27          |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.69377947 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 70          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 57          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.01712618  |\n",
            "|    clip_fraction        | 0.215       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0183     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.21        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0279     |\n",
            "|    reward               | -0.27466208 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 8.72        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 64          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 94          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017084152 |\n",
            "|    clip_fraction        | 0.205       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.0179     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.55        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0258     |\n",
            "|    reward               | -1.9653792  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 10.3        |\n",
            "-----------------------------------------\n",
            "day: 1761, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2182115.17\n",
            "total_reward: 1182115.17\n",
            "total_cost: 143085.96\n",
            "total_trades: 48732\n",
            "Sharpe: 0.838\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 62          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 131         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020034451 |\n",
            "|    clip_fraction        | 0.253       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.00108    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.08        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0265     |\n",
            "|    reward               | 0.010132103 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 10.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 62          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 162         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025925264 |\n",
            "|    clip_fraction        | 0.24        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | 0.0123      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.18        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0269     |\n",
            "|    reward               | -0.9151826  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 11.4        |\n",
            "-----------------------------------------\n",
            "======PPO Validation from:  2016-01-04 to  2016-04-05\n",
            "PPO Sharpe Ratio:  0.1618189683640768\n",
            "======DDPG Training========\n",
            "{'buffer_size': 100000, 'learning_rate': 5e-06, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_54\n",
            "day: 1761, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2657271.07\n",
            "total_reward: 1657271.07\n",
            "total_cost: 1790.60\n",
            "total_trades: 29322\n",
            "Sharpe: 0.940\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 40         |\n",
            "|    time_elapsed    | 174        |\n",
            "|    total timesteps | 7048       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | -168       |\n",
            "|    critic_loss     | 1.42e+03   |\n",
            "|    learning_rate   | 5e-06      |\n",
            "|    n_updates       | 5286       |\n",
            "|    reward          | -2.8536754 |\n",
            "-----------------------------------\n",
            "======DDPG Validation from:  2016-01-04 to  2016-04-05\n",
            "======Best Model Retraining from:  2009-01-01 to  2016-04-05\n",
            "======Trading from:  2016-04-05 to  2016-07-05\n",
            "============================================\n",
            "turbulence_threshold:  167.99271650842078\n",
            "======Model training from:  2009-01-01 to  2016-04-05\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0005}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_189_54\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 73          |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0.0687      |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -142        |\n",
            "|    reward             | 0.111467615 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 13.6        |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 73           |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 13           |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.2        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0005       |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | -85.2        |\n",
            "|    reward             | 0.0007585862 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 6.56         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 73        |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -73.7     |\n",
            "|    reward             | 10.282771 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 7.18      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 73         |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 27         |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | -0.03      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | 18.4       |\n",
            "|    reward             | 0.56108844 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 2.94       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 73        |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0.0429    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 197       |\n",
            "|    reward             | 5.4191055 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 27        |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -211      |\n",
            "|    reward             | 3.2580595 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 25.7      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 48         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -838       |\n",
            "|    reward             | -1.3153276 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 436        |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 72       |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 55       |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41      |\n",
            "|    explained_variance | -0.062   |\n",
            "|    learning_rate      | 0.0005   |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -191     |\n",
            "|    reward             | 0.927335 |\n",
            "|    std                | 0.995    |\n",
            "|    value_loss         | 23.7     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 62        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 47.9      |\n",
            "|    reward             | -2.334054 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 3.27      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 73         |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 68         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 79.5       |\n",
            "|    reward             | -3.8966372 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 7.04       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 73        |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 74        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -14.4     |\n",
            "|    reward             | 0.1586787 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 0.346     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 74         |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 81         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 2.38e-07   |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 2.96       |\n",
            "|    reward             | 0.20069031 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.706      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 74          |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 87          |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | 143         |\n",
            "|    reward             | -0.68389213 |\n",
            "|    std                | 0.999       |\n",
            "|    value_loss         | 16          |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 74          |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 93          |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | 311         |\n",
            "|    reward             | -0.39650825 |\n",
            "|    std                | 0.999       |\n",
            "|    value_loss         | 50.9        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 74        |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 100       |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -1.39     |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 86.7      |\n",
            "|    reward             | 1.9341949 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 6.94      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 75        |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 106       |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.0112    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 83.6      |\n",
            "|    reward             | 2.0586083 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 32.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 75        |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 112       |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -22.9     |\n",
            "|    reward             | 1.2526678 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 2.69      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 75       |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 119      |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.2    |\n",
            "|    explained_variance | 0.142    |\n",
            "|    learning_rate      | 0.0005   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | -512     |\n",
            "|    reward             | 5.329371 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 171      |\n",
            "------------------------------------\n",
            "day: 1824, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4270501.24\n",
            "total_reward: 3270501.24\n",
            "total_cost: 81381.30\n",
            "total_trades: 40660\n",
            "Sharpe: 1.096\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 75         |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 125        |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.0165    |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -138       |\n",
            "|    reward             | -1.5198747 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 10.9       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 75        |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 132       |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 103       |\n",
            "|    reward             | 1.9700066 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 7.6       |\n",
            "-------------------------------------\n",
            "======A2C Validation from:  2016-04-05 to  2016-07-05\n",
            "A2C Sharpe Ratio:  0.14144384283238948\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_189_54\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 91         |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 22         |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.0685104 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 82          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 49          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013882574 |\n",
            "|    clip_fraction        | 0.205       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00255    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.01        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0215     |\n",
            "|    reward               | 0.03327518  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 11          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 79          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 77          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013879539 |\n",
            "|    clip_fraction        | 0.181       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.0212      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.52        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.023      |\n",
            "|    reward               | 2.082618    |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 9.6         |\n",
            "-----------------------------------------\n",
            "day: 1824, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1940705.56\n",
            "total_reward: 940705.56\n",
            "total_cost: 151664.33\n",
            "total_trades: 50651\n",
            "Sharpe: 0.739\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 75          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 108         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018804017 |\n",
            "|    clip_fraction        | 0.212       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.0294     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.78        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0207     |\n",
            "|    reward               | 0.53654873  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 12          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 75          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 135         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020751122 |\n",
            "|    clip_fraction        | 0.198       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.00106    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.32        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0233     |\n",
            "|    reward               | 0.50879854  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 11.9        |\n",
            "-----------------------------------------\n",
            "======PPO Validation from:  2016-04-05 to  2016-07-05\n",
            "PPO Sharpe Ratio:  0.0032648038423752058\n",
            "======DDPG Training========\n",
            "{'buffer_size': 100000, 'learning_rate': 5e-06, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_54\n",
            "day: 1824, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2027311.92\n",
            "total_reward: 1027311.92\n",
            "total_cost: 1161.39\n",
            "total_trades: 31265\n",
            "Sharpe: 0.649\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 60          |\n",
            "|    time_elapsed    | 121         |\n",
            "|    total timesteps | 7300        |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 78.8        |\n",
            "|    critic_loss     | 187         |\n",
            "|    learning_rate   | 5e-06       |\n",
            "|    n_updates       | 5475        |\n",
            "|    reward          | -0.44087037 |\n",
            "------------------------------------\n",
            "======DDPG Validation from:  2016-04-05 to  2016-07-05\n",
            "======Best Model Retraining from:  2009-01-01 to  2016-07-05\n",
            "======Trading from:  2016-07-05 to  2016-10-03\n",
            "============================================\n",
            "turbulence_threshold:  167.99271650842078\n",
            "======Model training from:  2009-01-01 to  2016-07-05\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0005}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_252_54\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 53          |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0.279       |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -48.4       |\n",
            "|    reward             | -0.27091375 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.98        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 54         |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | -0.257     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 12.8       |\n",
            "|    reward             | -1.8003662 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 0.413      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 56        |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0.211     |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -153      |\n",
            "|    reward             | 2.2891326 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 13.9      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 54         |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 36         |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.0109     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | 65.7       |\n",
            "|    reward             | -2.5998461 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 5.26       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 53         |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 46         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.055     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -121       |\n",
            "|    reward             | -1.1054202 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 11.1       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 52         |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 57         |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.121      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -7.1       |\n",
            "|    reward             | -2.8148522 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.527      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 54          |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 64          |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | -0.054      |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | -86.8       |\n",
            "|    reward             | -0.32632175 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 4.97        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 55         |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 71         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.282      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 21.5       |\n",
            "|    reward             | -1.2139018 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.15       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 55        |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 81        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.0263   |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 95.1      |\n",
            "|    reward             | 0.7259401 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 5.49      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 53          |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 92          |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 14.7        |\n",
            "|    reward             | -0.19918434 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.5         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 54        |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 101       |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.00117   |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -169      |\n",
            "|    reward             | 1.8834113 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 25.2      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 54         |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 109        |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.0143     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -41.4      |\n",
            "|    reward             | -0.6300675 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 4.14       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 54        |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 119       |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -0.944    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 50.9      |\n",
            "|    reward             | 0.8778761 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 5.37      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 53        |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 130       |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0.188     |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 70.5      |\n",
            "|    reward             | -1.217825 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 6.36      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 53        |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 140       |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0.0576    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 134       |\n",
            "|    reward             | 2.5910356 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 10.9      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 54        |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 147       |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -0.364    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -2.52     |\n",
            "|    reward             | 1.0656178 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.52      |\n",
            "-------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 54            |\n",
            "|    iterations         | 1700          |\n",
            "|    time_elapsed       | 155           |\n",
            "|    total_timesteps    | 8500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -41.2         |\n",
            "|    explained_variance | -0.414        |\n",
            "|    learning_rate      | 0.0005        |\n",
            "|    n_updates          | 1699          |\n",
            "|    policy_loss        | -42.4         |\n",
            "|    reward             | -0.0066590775 |\n",
            "|    std                | 1             |\n",
            "|    value_loss         | 2.27          |\n",
            "-----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 54        |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 164       |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.156     |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -57.9     |\n",
            "|    reward             | 3.3482454 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 9.05      |\n",
            "-------------------------------------\n",
            "day: 1887, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3075452.56\n",
            "total_reward: 2075452.56\n",
            "total_cost: 119523.50\n",
            "total_trades: 45898\n",
            "Sharpe: 0.944\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 54        |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 174       |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 109       |\n",
            "|    reward             | 2.3156238 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 13.2      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 55        |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 181       |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.0217    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -60.9     |\n",
            "|    reward             | 1.6044157 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 6.26      |\n",
            "-------------------------------------\n",
            "======A2C Validation from:  2016-07-05 to  2016-10-03\n",
            "A2C Sharpe Ratio:  -0.034943215132653034\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_252_53\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 59        |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 34        |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 0.8232694 |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 60          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 67          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017805133 |\n",
            "|    clip_fraction        | 0.243       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0138     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.22        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0237     |\n",
            "|    reward               | 0.4098557   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 13          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 63          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 96          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017657172 |\n",
            "|    clip_fraction        | 0.203       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.0131      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.73        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0159     |\n",
            "|    reward               | 0.29493767  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 7.84        |\n",
            "-----------------------------------------\n",
            "day: 1887, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2237875.28\n",
            "total_reward: 1237875.28\n",
            "total_cost: 165854.23\n",
            "total_trades: 52805\n",
            "Sharpe: 0.812\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 61          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 132         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019104209 |\n",
            "|    clip_fraction        | 0.188       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.00446    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.43        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0209     |\n",
            "|    reward               | 1.6635935   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 9.45        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 62          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 163         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020057697 |\n",
            "|    clip_fraction        | 0.213       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.00405    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.7         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0226     |\n",
            "|    reward               | 1.0093099   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 10.5        |\n",
            "-----------------------------------------\n",
            "======PPO Validation from:  2016-07-05 to  2016-10-03\n",
            "PPO Sharpe Ratio:  0.004616075793198504\n",
            "======DDPG Training========\n",
            "{'buffer_size': 100000, 'learning_rate': 5e-06, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_53\n",
            "day: 1887, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2683162.23\n",
            "total_reward: 1683162.23\n",
            "total_cost: 1552.11\n",
            "total_trades: 33913\n",
            "Sharpe: 0.789\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 44          |\n",
            "|    time_elapsed    | 171         |\n",
            "|    total timesteps | 7552        |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 57.1        |\n",
            "|    critic_loss     | 103         |\n",
            "|    learning_rate   | 5e-06       |\n",
            "|    n_updates       | 5664        |\n",
            "|    reward          | 0.009220866 |\n",
            "------------------------------------\n",
            "======DDPG Validation from:  2016-07-05 to  2016-10-03\n",
            "======Best Model Retraining from:  2009-01-01 to  2016-10-03\n",
            "======Trading from:  2016-10-03 to  2017-01-03\n",
            "============================================\n",
            "turbulence_threshold:  167.99271650842078\n",
            "======Model training from:  2009-01-01 to  2016-10-03\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0005}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_315_53\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 55         |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.481     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -47.3      |\n",
            "|    reward             | 0.22725664 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.44       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 51         |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.23      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -38.3      |\n",
            "|    reward             | -1.0632675 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.29       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 52        |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -167      |\n",
            "|    reward             | 5.2655554 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 16.8      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 55         |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 36         |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | -108       |\n",
            "|    reward             | -1.1756636 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 7.75       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 57          |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 43          |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | -0.0944     |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | -73.6       |\n",
            "|    reward             | -0.08883824 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 3.53        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 57        |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 52        |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.0134    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 84.8      |\n",
            "|    reward             | 1.3361337 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 6.16      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 55        |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 63        |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 28.9      |\n",
            "|    reward             | 2.1405401 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.3       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 54        |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 74        |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 105       |\n",
            "|    reward             | 2.4207354 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 7.72      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 53        |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 84        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -76.6     |\n",
            "|    reward             | 0.8331413 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 2.96      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 52        |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 95        |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.00884  |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 200       |\n",
            "|    reward             | 1.9167218 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 25.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 51         |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 106        |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 164        |\n",
            "|    reward             | -0.4209981 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 16.1       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 52        |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 115       |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.334    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 90.8      |\n",
            "|    reward             | 0.1625947 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 6.38      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 52         |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 124        |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 117        |\n",
            "|    reward             | 0.48705262 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 8.58       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 53        |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 131       |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 160       |\n",
            "|    reward             | -0.296882 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 15        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 53         |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 140        |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 410        |\n",
            "|    reward             | -1.8597949 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 90.6       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 53         |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 149        |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.0784    |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 69.7       |\n",
            "|    reward             | -0.5220872 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 7.09       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 53        |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 159       |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 63.3      |\n",
            "|    reward             | 3.0878146 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 8.86      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 53          |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 168         |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | -28.5       |\n",
            "|    reward             | -0.47606778 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 7.68        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 53         |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 176        |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -205       |\n",
            "|    reward             | -0.1019032 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 33.7       |\n",
            "--------------------------------------\n",
            "day: 1950, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2792529.46\n",
            "total_reward: 1792529.46\n",
            "total_cost: 20730.71\n",
            "total_trades: 31281\n",
            "Sharpe: 0.808\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 54         |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 183        |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -9.79      |\n",
            "|    reward             | 0.56243616 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.595      |\n",
            "--------------------------------------\n",
            "======A2C Validation from:  2016-10-03 to  2017-01-03\n",
            "A2C Sharpe Ratio:  0.4746679509825115\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_315_53\n",
            "-------------------------------------\n",
            "| time/              |              |\n",
            "|    fps             | 63           |\n",
            "|    iterations      | 1            |\n",
            "|    time_elapsed    | 32           |\n",
            "|    total_timesteps | 2048         |\n",
            "| train/             |              |\n",
            "|    reward          | 0.0023785874 |\n",
            "-------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 62          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 65          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020136107 |\n",
            "|    clip_fraction        | 0.266       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.000508    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5           |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0246     |\n",
            "|    reward               | 0.43501925  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 9.42        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 62          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 98          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017699167 |\n",
            "|    clip_fraction        | 0.207       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.00298    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.43        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.025      |\n",
            "|    reward               | 0.38440135  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 9.41        |\n",
            "-----------------------------------------\n",
            "day: 1950, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2026434.30\n",
            "total_reward: 1026434.30\n",
            "total_cost: 168199.14\n",
            "total_trades: 54074\n",
            "Sharpe: 0.701\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 60          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 134         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020897683 |\n",
            "|    clip_fraction        | 0.227       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.02       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.71        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0216     |\n",
            "|    reward               | 1.2194041   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 12          |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 60         |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 168        |\n",
            "|    total_timesteps      | 10240      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02338132 |\n",
            "|    clip_fraction        | 0.274      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.5      |\n",
            "|    explained_variance   | -0.0107    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 5.38       |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.0214    |\n",
            "|    reward               | -0.2955554 |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 12.2       |\n",
            "----------------------------------------\n",
            "======PPO Validation from:  2016-10-03 to  2017-01-03\n",
            "PPO Sharpe Ratio:  0.27582929709939225\n",
            "======DDPG Training========\n",
            "{'buffer_size': 100000, 'learning_rate': 5e-06, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_53\n",
            "day: 1950, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2558709.87\n",
            "total_reward: 1558709.87\n",
            "total_cost: 1306.60\n",
            "total_trades: 27397\n",
            "Sharpe: 0.702\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 42       |\n",
            "|    time_elapsed    | 183      |\n",
            "|    total timesteps | 7804     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 97.2     |\n",
            "|    critic_loss     | 446      |\n",
            "|    learning_rate   | 5e-06    |\n",
            "|    n_updates       | 5853     |\n",
            "|    reward          | 2.65782  |\n",
            "---------------------------------\n",
            "======DDPG Validation from:  2016-10-03 to  2017-01-03\n",
            "======Best Model Retraining from:  2009-01-01 to  2017-01-03\n",
            "======Trading from:  2017-01-03 to  2017-04-04\n",
            "============================================\n",
            "turbulence_threshold:  167.99271650842078\n",
            "======Model training from:  2009-01-01 to  2017-01-03\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0005}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_378_51\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 80          |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | -1.52       |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | 6.56        |\n",
            "|    reward             | 0.052844685 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.782       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 80         |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -18.1      |\n",
            "|    reward             | -1.6024219 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.409      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 80        |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.125     |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -141      |\n",
            "|    reward             | 3.4920428 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 14.5      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 80        |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.0474   |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 190       |\n",
            "|    reward             | 2.3292363 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 23.7      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 80          |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 31          |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0.139       |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | -40.9       |\n",
            "|    reward             | -0.71532315 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.88        |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 79           |\n",
            "|    iterations         | 600          |\n",
            "|    time_elapsed       | 37           |\n",
            "|    total_timesteps    | 3000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.2        |\n",
            "|    explained_variance | -0.031       |\n",
            "|    learning_rate      | 0.0005       |\n",
            "|    n_updates          | 599          |\n",
            "|    policy_loss        | 110          |\n",
            "|    reward             | -0.031861532 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 12.4         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 79        |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 18.4      |\n",
            "|    reward             | 1.9727415 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.625     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 79        |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 50        |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 194       |\n",
            "|    reward             | 1.4322522 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 31.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 79        |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 56        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.00601  |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -4.4      |\n",
            "|    reward             | -3.189838 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.38      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 79         |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 62         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 16.3       |\n",
            "|    reward             | -1.1385738 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.671      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 80        |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 68        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.0145    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 90.7      |\n",
            "|    reward             | 2.9167871 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 10.5      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 80          |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 74          |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | 191         |\n",
            "|    reward             | -0.45086673 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 30.3        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 80         |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 80         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.454     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 64.4       |\n",
            "|    reward             | -0.7828792 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.35       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 80         |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 86         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -7.15e-07  |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -137       |\n",
            "|    reward             | -2.5422888 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 15.9       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 80        |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 93        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -167      |\n",
            "|    reward             | 2.6314387 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 18.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 78         |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 101        |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -0.502     |\n",
            "|    reward             | -1.0951563 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.22       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 77         |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 109        |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.00836   |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -29.5      |\n",
            "|    reward             | 0.21161316 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.29       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 75        |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 119       |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -3.15     |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -20.5     |\n",
            "|    reward             | 1.2658947 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.639     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 130        |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.211     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -101       |\n",
            "|    reward             | -1.4470869 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 5.52       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 138       |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -0.0211   |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -115      |\n",
            "|    reward             | 3.1263506 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 20.3      |\n",
            "-------------------------------------\n",
            "======A2C Validation from:  2017-01-03 to  2017-04-04\n",
            "A2C Sharpe Ratio:  0.21025508447642435\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_378_51\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 85          |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 23          |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.67454696 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 81          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 50          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015315263 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0109     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.33        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0194     |\n",
            "|    reward               | 0.2844316   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 12          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 74          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 82          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018249322 |\n",
            "|    clip_fraction        | 0.214       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.0338     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.23        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0239     |\n",
            "|    reward               | 1.328936    |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 10.6        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 71         |\n",
            "|    iterations           | 4          |\n",
            "|    time_elapsed         | 114        |\n",
            "|    total_timesteps      | 8192       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01804297 |\n",
            "|    clip_fraction        | 0.229      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.3      |\n",
            "|    explained_variance   | -0.00631   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 3.81       |\n",
            "|    n_updates            | 30         |\n",
            "|    policy_gradient_loss | -0.0254    |\n",
            "|    reward               | 0.58120435 |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 12.1       |\n",
            "----------------------------------------\n",
            "day: 2013, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2494972.94\n",
            "total_reward: 1494972.94\n",
            "total_cost: 174859.46\n",
            "total_trades: 55808\n",
            "Sharpe: 0.850\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 68          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 149         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025528591 |\n",
            "|    clip_fraction        | 0.223       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.0184     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.92        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0284     |\n",
            "|    reward               | 0.77323556  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 15.7        |\n",
            "-----------------------------------------\n",
            "======PPO Validation from:  2017-01-03 to  2017-04-04\n",
            "PPO Sharpe Ratio:  0.21684487122220164\n",
            "======DDPG Training========\n",
            "{'buffer_size': 100000, 'learning_rate': 5e-06, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_378_51\n",
            "day: 2013, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2848358.73\n",
            "total_reward: 1848358.73\n",
            "total_cost: 1143.62\n",
            "total_trades: 37834\n",
            "Sharpe: 0.857\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 40         |\n",
            "|    time_elapsed    | 200        |\n",
            "|    total timesteps | 8056       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 87.8       |\n",
            "|    critic_loss     | 202        |\n",
            "|    learning_rate   | 5e-06      |\n",
            "|    n_updates       | 6042       |\n",
            "|    reward          | -0.7894779 |\n",
            "-----------------------------------\n",
            "======DDPG Validation from:  2017-01-03 to  2017-04-04\n",
            "======Best Model Retraining from:  2009-01-01 to  2017-04-04\n",
            "======Trading from:  2017-04-04 to  2017-07-05\n",
            "============================================\n",
            "turbulence_threshold:  167.99271650842078\n",
            "======Model training from:  2009-01-01 to  2017-04-04\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0005}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_441_51\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 45        |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -0.144    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | -1.72     |\n",
            "|    reward             | 0.2754093 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.125     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 46        |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -0.159    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | -19.3     |\n",
            "|    reward             | -1.422748 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.872     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 48        |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.13      |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -166      |\n",
            "|    reward             | 2.7707505 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 17.5      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 49         |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 40         |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.15      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | -43.2      |\n",
            "|    reward             | 0.38329396 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.71       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 48         |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 51         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.0839     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -60.8      |\n",
            "|    reward             | 0.78281605 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.83       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 47         |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 62         |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.524      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -177       |\n",
            "|    reward             | -1.4700135 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 18.4       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 47         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 73         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.362     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 167        |\n",
            "|    reward             | -1.1726744 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 23.5       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 47         |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 83         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | -0.0885    |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 36.6       |\n",
            "|    reward             | 0.23679067 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.24       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 47         |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 94         |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | -0.0657    |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 178        |\n",
            "|    reward             | -1.2512053 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 22.6       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 47          |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 105         |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | -108        |\n",
            "|    reward             | -0.12042166 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 7.3         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 46        |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 117       |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 59.1      |\n",
            "|    reward             | 1.1429645 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 2.56      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 46         |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 127        |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | -0.00465   |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 280        |\n",
            "|    reward             | -1.7420615 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 49.9       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 46         |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 138        |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | -0.0172    |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -128       |\n",
            "|    reward             | -1.4488627 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 10.5       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 46        |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 149       |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0.112     |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 28.4      |\n",
            "|    reward             | -0.524536 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 1.47      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 46          |\n",
            "|    iterations         | 1500        |\n",
            "|    time_elapsed       | 159         |\n",
            "|    total_timesteps    | 7500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 1499        |\n",
            "|    policy_loss        | 67.4        |\n",
            "|    reward             | -0.21450055 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 5.78        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 46         |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 171        |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0.00837    |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -382       |\n",
            "|    reward             | -7.9194045 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 107        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 47        |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 180       |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -0.0813   |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -6.59     |\n",
            "|    reward             | 0.4669253 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 0.842     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 47         |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 189        |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | -0.0229    |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -335       |\n",
            "|    reward             | -3.6841218 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 97.1       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 48         |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 197        |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -18.9      |\n",
            "|    reward             | -1.5006676 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 3.42       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 48         |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 206        |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0.122      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 201        |\n",
            "|    reward             | -1.2710164 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 24.1       |\n",
            "--------------------------------------\n",
            "======A2C Validation from:  2017-04-04 to  2017-07-05\n",
            "A2C Sharpe Ratio:  0.22686310223090425\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_441_51\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 74         |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 27         |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | 0.33995274 |\n",
            "-----------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 55         |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01822669 |\n",
            "|    clip_fraction        | 0.245      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.2      |\n",
            "|    explained_variance   | -0.0113    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 4.04       |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.0319    |\n",
            "|    reward               | -0.7160729 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 11.9       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 74          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 82          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022183534 |\n",
            "|    clip_fraction        | 0.24        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00968    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.49        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.028      |\n",
            "|    reward               | -0.35477436 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 10.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 74          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 109         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026757486 |\n",
            "|    clip_fraction        | 0.279       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.00528     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.27        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0301     |\n",
            "|    reward               | -1.097287   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 9.42        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 75          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 136         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023455359 |\n",
            "|    clip_fraction        | 0.261       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | 0.00612     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.11        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0235     |\n",
            "|    reward               | 1.3471042   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 14.7        |\n",
            "-----------------------------------------\n",
            "======PPO Validation from:  2017-04-04 to  2017-07-05\n",
            "PPO Sharpe Ratio:  0.0794070573452093\n",
            "======DDPG Training========\n",
            "{'buffer_size': 100000, 'learning_rate': 5e-06, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_441_51\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 54         |\n",
            "|    time_elapsed    | 151        |\n",
            "|    total timesteps | 8308       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | -112       |\n",
            "|    critic_loss     | 347        |\n",
            "|    learning_rate   | 5e-06      |\n",
            "|    n_updates       | 6231       |\n",
            "|    reward          | -0.1997885 |\n",
            "-----------------------------------\n",
            "day: 2076, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2975600.98\n",
            "total_reward: 1975600.98\n",
            "total_cost: 1284.91\n",
            "total_trades: 25014\n",
            "Sharpe: 0.803\n",
            "=================================\n",
            "======DDPG Validation from:  2017-04-04 to  2017-07-05\n",
            "======Best Model Retraining from:  2009-01-01 to  2017-07-05\n",
            "======Trading from:  2017-07-05 to  2017-10-03\n",
            "============================================\n",
            "turbulence_threshold:  167.99271650842078\n",
            "======Model training from:  2009-01-01 to  2017-07-05\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0005}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_504_51\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 69           |\n",
            "|    iterations         | 100          |\n",
            "|    time_elapsed       | 7            |\n",
            "|    total_timesteps    | 500          |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.2        |\n",
            "|    explained_variance | -0.421       |\n",
            "|    learning_rate      | 0.0005       |\n",
            "|    n_updates          | 99           |\n",
            "|    policy_loss        | -28.2        |\n",
            "|    reward             | 0.0044368776 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 0.582        |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 70          |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 14          |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | -0.498      |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | -52.5       |\n",
            "|    reward             | -0.92339367 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 2.53        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 70        |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -560      |\n",
            "|    reward             | 2.1290843 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 192       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 69        |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 35.7      |\n",
            "|    reward             | 1.0878066 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 4.04      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 68        |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 54        |\n",
            "|    reward             | 0.2704809 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 5.19      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 68        |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 43        |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 68.5      |\n",
            "|    reward             | -4.852266 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 12.8      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 52         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.0101     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -125       |\n",
            "|    reward             | -1.4909266 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 14.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 66        |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 60        |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -1.55e-06 |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -74.2     |\n",
            "|    reward             | 1.8076167 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 7.95      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 65        |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 68        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.0334   |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 188       |\n",
            "|    reward             | 2.4443426 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 24.5      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 65       |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 76       |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.1    |\n",
            "|    explained_variance | -0.036   |\n",
            "|    learning_rate      | 0.0005   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | -130     |\n",
            "|    reward             | -5.52    |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 21       |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 65         |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 84         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -15.6      |\n",
            "|    reward             | 0.21210961 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 1.29       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 64        |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 92        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 284       |\n",
            "|    reward             | 2.1967301 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 83.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 64        |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 100       |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -21.7     |\n",
            "|    reward             | 1.8583825 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.5       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 64         |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 108        |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -48        |\n",
            "|    reward             | 0.80937314 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.08       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 64        |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 115       |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -0.0207   |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 96.7      |\n",
            "|    reward             | 2.5732331 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 7.37      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 65        |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 122       |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0.0167    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 97.1      |\n",
            "|    reward             | 1.8883451 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 8.34      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 65          |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 129         |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | -24.4       |\n",
            "|    reward             | -0.27164865 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 1.35        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 65          |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 137         |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | -0.0721     |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | 251         |\n",
            "|    reward             | -0.13505194 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 41.1        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 65         |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 144        |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 173        |\n",
            "|    reward             | -5.0355773 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 29.6       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 65         |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 151        |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 3.71       |\n",
            "|    reward             | -2.0496244 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.2        |\n",
            "--------------------------------------\n",
            "======A2C Validation from:  2017-07-05 to  2017-10-03\n",
            "A2C Sharpe Ratio:  0.5718612234124743\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_504_51\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 79        |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 25        |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 0.6456228 |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 76          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 53          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019973855 |\n",
            "|    clip_fraction        | 0.261       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0371     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.02        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0279     |\n",
            "|    reward               | 0.59987086  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 9.75        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 75          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 81          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018533254 |\n",
            "|    clip_fraction        | 0.232       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.00359     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.97        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0313     |\n",
            "|    reward               | -1.0312163  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 11.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 75          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 108         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022184215 |\n",
            "|    clip_fraction        | 0.222       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.00431    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.56        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0271     |\n",
            "|    reward               | 0.1953454   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 13.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 74          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 136         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025493361 |\n",
            "|    clip_fraction        | 0.273       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.00386    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.56        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0278     |\n",
            "|    reward               | -3.1449509  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 10.5        |\n",
            "-----------------------------------------\n",
            "======PPO Validation from:  2017-07-05 to  2017-10-03\n",
            "PPO Sharpe Ratio:  0.20553476568003523\n",
            "======DDPG Training========\n",
            "{'buffer_size': 100000, 'learning_rate': 5e-06, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_504_51\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 54        |\n",
            "|    time_elapsed    | 158       |\n",
            "|    total timesteps | 8560      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 122       |\n",
            "|    critic_loss     | 853       |\n",
            "|    learning_rate   | 5e-06     |\n",
            "|    n_updates       | 6420      |\n",
            "|    reward          | 2.1068087 |\n",
            "----------------------------------\n",
            "day: 2139, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2944815.39\n",
            "total_reward: 1944815.39\n",
            "total_cost: 1750.56\n",
            "total_trades: 35120\n",
            "Sharpe: 0.872\n",
            "=================================\n",
            "======DDPG Validation from:  2017-07-05 to  2017-10-03\n",
            "======Best Model Retraining from:  2009-01-01 to  2017-10-03\n",
            "======Trading from:  2017-10-03 to  2018-01-03\n",
            "============================================\n",
            "turbulence_threshold:  167.99271650842078\n",
            "======Model training from:  2009-01-01 to  2017-10-03\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0005}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_567_51\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 66         |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | -1.03      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -29.8      |\n",
            "|    reward             | -0.2929793 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 1.48       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | -0.57      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 1.87       |\n",
            "|    reward             | -0.6470796 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 1.12       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 67       |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.1    |\n",
            "|    explained_variance | -0.786   |\n",
            "|    learning_rate      | 0.0005   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | -270     |\n",
            "|    reward             | 4.139986 |\n",
            "|    std                | 0.997    |\n",
            "|    value_loss         | 36.5     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 67        |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0.0144    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 44.2      |\n",
            "|    reward             | 2.5602343 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 1.65      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 67        |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -1.44     |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -3.07     |\n",
            "|    reward             | 0.5822587 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 0.729     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 44         |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.266     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -15.6      |\n",
            "|    reward             | -0.3340775 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.364      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 51         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.362     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -16.4      |\n",
            "|    reward             | 0.29390046 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.51       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 67        |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 58        |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 454       |\n",
            "|    reward             | 1.3742793 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 146       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 68          |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 66          |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -9.71       |\n",
            "|    reward             | -0.37390932 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.298       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 68        |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 73        |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0.00252   |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -64.3     |\n",
            "|    reward             | 1.0516827 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 2.43      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 68         |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 80         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0.133      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -41.1      |\n",
            "|    reward             | -1.8987138 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.28       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 68          |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 87          |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.3       |\n",
            "|    explained_variance | -0.0872     |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | 65          |\n",
            "|    reward             | -0.54458463 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 7.15        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 95         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -139       |\n",
            "|    reward             | -1.0426972 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 17.5       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 67       |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 103      |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.3    |\n",
            "|    explained_variance | 0.147    |\n",
            "|    learning_rate      | 0.0005   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -102     |\n",
            "|    reward             | 1.209213 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 6.35     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 67        |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 110       |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0.0261    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -37.1     |\n",
            "|    reward             | 1.9032989 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.5       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 68         |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 117        |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.00762    |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -43.6      |\n",
            "|    reward             | -0.9570238 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.46       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 68        |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 124       |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -0.00982  |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 495       |\n",
            "|    reward             | 1.6889431 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 183       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 68         |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 131        |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -3.81      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -26        |\n",
            "|    reward             | -2.8158422 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.38       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 68        |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 139       |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0.125     |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 67.5      |\n",
            "|    reward             | 3.1952171 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 5.01      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 68         |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 146        |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | -2.6       |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 27.1       |\n",
            "|    reward             | 0.44854844 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.809      |\n",
            "--------------------------------------\n",
            "======A2C Validation from:  2017-10-03 to  2018-01-03\n",
            "A2C Sharpe Ratio:  0.38030791558682464\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_567_51\n",
            "-------------------------------------\n",
            "| time/              |              |\n",
            "|    fps             | 81           |\n",
            "|    iterations      | 1            |\n",
            "|    time_elapsed    | 25           |\n",
            "|    total_timesteps | 2048         |\n",
            "| train/             |              |\n",
            "|    reward          | 0.0031986488 |\n",
            "-------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 75          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 53          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018119391 |\n",
            "|    clip_fraction        | 0.235       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0175     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.04        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0216     |\n",
            "|    reward               | 0.57657456  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 8.06        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 74          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 82          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017090734 |\n",
            "|    clip_fraction        | 0.213       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.00555     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.97        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0243     |\n",
            "|    reward               | -0.2960566  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 10.5        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 4          |\n",
            "|    time_elapsed         | 110        |\n",
            "|    total_timesteps      | 8192       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02004273 |\n",
            "|    clip_fraction        | 0.21       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.3      |\n",
            "|    explained_variance   | -0.0116    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 3.03       |\n",
            "|    n_updates            | 30         |\n",
            "|    policy_gradient_loss | -0.023     |\n",
            "|    reward               | 1.9464791  |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 11         |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 139         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024177935 |\n",
            "|    clip_fraction        | 0.259       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.0149      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.6         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0225     |\n",
            "|    reward               | 0.7865473   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 10.4        |\n",
            "-----------------------------------------\n",
            "======PPO Validation from:  2017-10-03 to  2018-01-03\n",
            "PPO Sharpe Ratio:  0.5050886700022779\n",
            "======DDPG Training========\n",
            "{'buffer_size': 100000, 'learning_rate': 5e-06, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_567_51\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 53        |\n",
            "|    time_elapsed    | 164       |\n",
            "|    total timesteps | 8812      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -167      |\n",
            "|    critic_loss     | 444       |\n",
            "|    learning_rate   | 5e-06     |\n",
            "|    n_updates       | 6609      |\n",
            "|    reward          | 1.6522355 |\n",
            "----------------------------------\n",
            "day: 2202, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3520437.90\n",
            "total_reward: 2520437.90\n",
            "total_cost: 2692.75\n",
            "total_trades: 24207\n",
            "Sharpe: 1.005\n",
            "=================================\n",
            "======DDPG Validation from:  2017-10-03 to  2018-01-03\n",
            "======Best Model Retraining from:  2009-01-01 to  2018-01-03\n",
            "======Trading from:  2018-01-03 to  2018-04-05\n",
            "============================================\n",
            "turbulence_threshold:  167.99271650842078\n",
            "======Model training from:  2009-01-01 to  2018-01-03\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0005}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_630_51\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 66         |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.331     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -24.3      |\n",
            "|    reward             | -0.2936716 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.56       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 66         |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.574     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -33.5      |\n",
            "|    reward             | -0.7017686 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.24       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 66       |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.2    |\n",
            "|    explained_variance | 0.193    |\n",
            "|    learning_rate      | 0.0005   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | -177     |\n",
            "|    reward             | 3.379107 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 21.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 67       |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.2    |\n",
            "|    explained_variance | 0.0496   |\n",
            "|    learning_rate      | 0.0005   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 9.31     |\n",
            "|    reward             | 1.389677 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.387    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 36         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.0437     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -10.6      |\n",
            "|    reward             | -1.0407755 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.75       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 67       |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 44       |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.2    |\n",
            "|    explained_variance | -0.115   |\n",
            "|    learning_rate      | 0.0005   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | -135     |\n",
            "|    reward             | 4.546327 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 14.7     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 68         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 51         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 49.6       |\n",
            "|    reward             | 0.43300536 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.35       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 68        |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 58        |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.0288    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -389      |\n",
            "|    reward             | 2.2385962 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 97.1      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 68        |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 66        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 98.7      |\n",
            "|    reward             | -2.819214 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 8.26      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 73         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 19.5       |\n",
            "|    reward             | -1.3437498 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.4        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 67        |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 80        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -85.2     |\n",
            "|    reward             | 2.2902832 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 6.57      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 68        |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 87        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 57.3      |\n",
            "|    reward             | 1.9940524 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 9.18      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 68       |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 94       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.3    |\n",
            "|    explained_variance | 0.0582   |\n",
            "|    learning_rate      | 0.0005   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 161      |\n",
            "|    reward             | 9.759446 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 15.6     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 68         |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 102        |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0.147      |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 70.3       |\n",
            "|    reward             | -2.5880246 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 3.07       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 68        |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 109       |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 159       |\n",
            "|    reward             | 2.0844252 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 27.9      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 68         |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 117        |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 69.2       |\n",
            "|    reward             | 0.40244645 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 5.88       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 68        |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 124       |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0.00867   |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 100       |\n",
            "|    reward             | 4.9939485 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 17.9      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 68       |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 131      |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 0.00156  |\n",
            "|    learning_rate      | 0.0005   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | -29      |\n",
            "|    reward             | 4.485044 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 8.33     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 139        |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 98.7       |\n",
            "|    reward             | -1.2745359 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 6.26       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 147        |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -6.66      |\n",
            "|    reward             | -0.6746286 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.47       |\n",
            "--------------------------------------\n",
            "======A2C Validation from:  2018-01-03 to  2018-04-05\n",
            "A2C Sharpe Ratio:  -0.0767633821583807\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_630_51\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 81          |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 25          |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.29040584 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 76          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 53          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020692324 |\n",
            "|    clip_fraction        | 0.243       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00649    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.53        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0285     |\n",
            "|    reward               | 1.8412149   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 13.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 75          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 81          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021421613 |\n",
            "|    clip_fraction        | 0.254       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.0109      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.11        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0244     |\n",
            "|    reward               | 0.30330995  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 14.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 74          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 110         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027283233 |\n",
            "|    clip_fraction        | 0.267       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.00384    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.02        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0275     |\n",
            "|    reward               | 1.6257758   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 11.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 74          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 137         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020281082 |\n",
            "|    clip_fraction        | 0.212       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | 0.0361      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.61        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0266     |\n",
            "|    reward               | 0.16750653  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 17.6        |\n",
            "-----------------------------------------\n",
            "======PPO Validation from:  2018-01-03 to  2018-04-05\n",
            "PPO Sharpe Ratio:  -0.1535228486948589\n",
            "======DDPG Training========\n",
            "{'buffer_size': 100000, 'learning_rate': 5e-06, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_630_51\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 53        |\n",
            "|    time_elapsed    | 168       |\n",
            "|    total timesteps | 9064      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 55.8      |\n",
            "|    critic_loss     | 474       |\n",
            "|    learning_rate   | 5e-06     |\n",
            "|    n_updates       | 6798      |\n",
            "|    reward          | 2.5719447 |\n",
            "----------------------------------\n",
            "day: 2265, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3220462.60\n",
            "total_reward: 2220462.60\n",
            "total_cost: 1705.53\n",
            "total_trades: 29661\n",
            "Sharpe: 0.813\n",
            "=================================\n",
            "======DDPG Validation from:  2018-01-03 to  2018-04-05\n",
            "======Best Model Retraining from:  2009-01-01 to  2018-04-05\n",
            "======Trading from:  2018-04-05 to  2018-07-05\n",
            "============================================\n",
            "turbulence_threshold:  167.99271650842078\n",
            "======Model training from:  2009-01-01 to  2018-04-05\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0005}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_693_51\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 68         |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.586     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -92.6      |\n",
            "|    reward             | 0.11701672 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 5.19       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 67        |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -0.00158  |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | -131      |\n",
            "|    reward             | -1.133674 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 11.9      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 67       |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0005   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | -135     |\n",
            "|    reward             | 4.341296 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 11.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 65       |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 30       |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0005   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | -62      |\n",
            "|    reward             | 3.773141 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.42     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 66        |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -0.245    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -25.1     |\n",
            "|    reward             | 0.8697647 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.474     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 66         |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 44         |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 169        |\n",
            "|    reward             | 0.96116805 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 20.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 67        |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 52        |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -110      |\n",
            "|    reward             | 2.5492024 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 10.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 59         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 36.8       |\n",
            "|    reward             | -7.1377683 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.54       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 67        |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 66        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -48.9     |\n",
            "|    reward             | -2.746629 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.45      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 73         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | -0.266     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -292       |\n",
            "|    reward             | -1.5222195 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 53         |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 67          |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 81          |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | 11.8        |\n",
            "|    reward             | -0.35261056 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.563       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 67        |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 88        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 28.1      |\n",
            "|    reward             | 0.8064424 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.633     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 95         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 79.4       |\n",
            "|    reward             | -1.5733273 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 6.01       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 68          |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 102         |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0005      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | -25.4       |\n",
            "|    reward             | -0.30653498 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.388       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 68        |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 110       |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 9.14      |\n",
            "|    reward             | 1.3471762 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.57      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 67        |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 117       |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0.164     |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 319       |\n",
            "|    reward             | 0.6639276 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 78.5      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 67        |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 125       |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -13.4     |\n",
            "|    reward             | 4.264751  |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.16      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 132        |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | -0.0675    |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 3.57       |\n",
            "|    reward             | 0.29320782 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.65       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 140        |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0.0294     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 66         |\n",
            "|    reward             | -2.5474372 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 4.21       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 147        |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | -0.0611    |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -65.5      |\n",
            "|    reward             | -0.3312138 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 7.99       |\n",
            "--------------------------------------\n",
            "======A2C Validation from:  2018-04-05 to  2018-07-05\n",
            "A2C Sharpe Ratio:  -0.0460423850201877\n",
            "======PPO Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ppo/ppo_693_51\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 76         |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 26         |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | 0.18204221 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 55          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017424904 |\n",
            "|    clip_fraction        | 0.22        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0213     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.97        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0249     |\n",
            "|    reward               | 0.61109686  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 11.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 85          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018579075 |\n",
            "|    clip_fraction        | 0.209       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.00557    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.09        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0252     |\n",
            "|    reward               | 0.56381017  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 13.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 71          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 114         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017245512 |\n",
            "|    clip_fraction        | 0.198       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.0384     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.69        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0228     |\n",
            "|    reward               | 2.429828    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 15          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 71          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 142         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018875666 |\n",
            "|    clip_fraction        | 0.195       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.000489   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.21        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0177     |\n",
            "|    reward               | 1.0118989   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 17.9        |\n",
            "-----------------------------------------\n",
            "======PPO Validation from:  2018-04-05 to  2018-07-05\n",
            "PPO Sharpe Ratio:  -0.20502362054937048\n",
            "======DDPG Training========\n",
            "{'buffer_size': 100000, 'learning_rate': 5e-06, 'batch_size': 64}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/ddpg/ddpg_693_51\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 52        |\n",
            "|    time_elapsed    | 177       |\n",
            "|    total timesteps | 9316      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -114      |\n",
            "|    critic_loss     | 272       |\n",
            "|    learning_rate   | 5e-06     |\n",
            "|    n_updates       | 6987      |\n",
            "|    reward          | 2.6383085 |\n",
            "----------------------------------\n",
            "day: 2328, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2655549.74\n",
            "total_reward: 1655549.74\n",
            "total_cost: 1917.02\n",
            "total_trades: 36296\n",
            "Sharpe: 0.700\n",
            "=================================\n",
            "======DDPG Validation from:  2018-04-05 to  2018-07-05\n",
            "======Best Model Retraining from:  2009-01-01 to  2018-07-05\n",
            "======Trading from:  2018-07-05 to  2018-10-03\n",
            "============================================\n",
            "turbulence_threshold:  167.99271650842078\n",
            "======Model training from:  2009-01-01 to  2018-07-05\n",
            "======A2C Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0005}\n",
            "Using cuda device\n",
            "Logging to tensorboard_log/a2c/a2c_756_51\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 62         |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.769     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -25.8      |\n",
            "|    reward             | 0.16829604 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.518      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 64         |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.104     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -25        |\n",
            "|    reward             | -1.2665474 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.927      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 64        |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0.0272    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -326      |\n",
            "|    reward             | 6.1139283 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 68.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 64        |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0.324     |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 73.6      |\n",
            "|    reward             | 1.3332148 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 3.59      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 64         |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 38         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.977     |\n",
            "|    learning_rate      | 0.0005     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 156        |\n",
            "|    reward             | 0.21220769 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 19.5       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 64        |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 46        |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -0.923    |\n",
            "|    learning_rate      | 0.0005    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -13.9     |\n",
            "|    reward             | 0.5685734 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.46      |\n",
            "-------------------------------------\n"
          ]
        }
      ],
      "source": [
        "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
        "                                                 PPO_model_kwargs,\n",
        "                                                 DDPG_model_kwargs,\n",
        "                                                 timesteps_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "-0qd8acMtj1f",
        "outputId": "c168ea80-66e1-4d3d-9fc9-dbcf0aea76f1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Iter</th>\n",
              "      <th>Val Start</th>\n",
              "      <th>Val End</th>\n",
              "      <th>Model Used</th>\n",
              "      <th>A2C Sharpe</th>\n",
              "      <th>PPO Sharpe</th>\n",
              "      <th>DDPG Sharpe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>126</td>\n",
              "      <td>2020-04-02</td>\n",
              "      <td>2020-07-02</td>\n",
              "      <td>PPO</td>\n",
              "      <td>0.142976</td>\n",
              "      <td>0.202735</td>\n",
              "      <td>0.177919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>189</td>\n",
              "      <td>2020-07-02</td>\n",
              "      <td>2020-10-01</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>0.103536</td>\n",
              "      <td>0.046765</td>\n",
              "      <td>0.177396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>252</td>\n",
              "      <td>2020-10-01</td>\n",
              "      <td>2020-12-31</td>\n",
              "      <td>DDPG</td>\n",
              "      <td>0.263445</td>\n",
              "      <td>0.311626</td>\n",
              "      <td>0.338765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>315</td>\n",
              "      <td>2020-12-31</td>\n",
              "      <td>2021-04-05</td>\n",
              "      <td>PPO</td>\n",
              "      <td>0.038233</td>\n",
              "      <td>0.278605</td>\n",
              "      <td>0.266208</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Iter   Val Start     Val End Model Used A2C Sharpe PPO Sharpe DDPG Sharpe\n",
              "0  126  2020-04-02  2020-07-02        PPO   0.142976   0.202735    0.177919\n",
              "1  189  2020-07-02  2020-10-01       DDPG   0.103536   0.046765    0.177396\n",
              "2  252  2020-10-01  2020-12-31       DDPG   0.263445   0.311626    0.338765\n",
              "3  315  2020-12-31  2021-04-05        PPO   0.038233   0.278605    0.266208"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6vvNSC6h1jZ"
      },
      "source": [
        "<a id='6'></a>\n",
        "# Part 7: Backtest Our Strategy\n",
        "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4JKB--8tj1g"
      },
      "outputs": [],
      "source": [
        "unique_trade_date = processed[(processed.date > val_test_start)&(processed.date <= val_test_end)].date.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "def compare(A, B): # 名字可以随便取，不一定得是“compare\"\n",
        "    return -1 if int(A.split(\"_\")[-1][:-4])<int(B.split(\"_\")[-1][:-4]) else 1\n",
        "\n",
        "from typing import List\n",
        "with open(\"paths.txt\") as f:\n",
        "    paths: List[str] = f.readlines()\n",
        "    paths = [path.strip().split(\" \")[-1] for path in paths]\n",
        "\n",
        "    paths.sort(key=functools.cmp_to_key(compare))\n",
        "    #paths.sort()\n",
        "#print(paths)\n",
        "\n",
        "df_account_value=pd.DataFrame()\n",
        "for i in range(len(df_summary)):\n",
        "    iter = df_summary.iloc[i][\"Iter\"]\n",
        "    al = df_summary.iloc[i][\"Model Used\"]\n",
        "    path = f\"results/account_value_validation_{al}_{iter}.csv\"\n",
        "    #print(path, os.path.exists(path))\n",
        "    df_tmp = pd.read_csv(path)\n",
        "    df_account_value = df_account_value.append(df_tmp,ignore_index=True)\n",
        "df_account_value\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_account_value.to_csv(\"results/account_value_all.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_account_value.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "df_account_value.account_value.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9mKF7GGtj1g",
        "outputId": "b348adf6-a0bf-423e-ca83-71472ba6fcf4",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sharpe Ratio:  2.5336892671034508\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime, timedelta\n",
        "time_current = datetime.now() + timedelta(hours=8)\n",
        "time_current = time_current.strftime(\"%Y-%m-%d_%H:%M\")\n",
        "\n",
        "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
        "\n",
        "csv_name_func_processed = lambda time: f\"./datasets/ensemble_{time}_processed.csv\"\n",
        "SAVE_PATH = csv_name_func_processed(time_current)\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    df_account_value = pd.read_csv(SAVE_PATH)\n",
        "else:\n",
        "    df_account_value=pd.DataFrame()\n",
        "    for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
        "        temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
        "        df_account_value = df_account_value.append(temp,ignore_index=True)\n",
        "    sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
        "    print('Sharpe Ratio: ',sharpe)\n",
        "    df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))\n",
        "    df_account_value.to_csv(SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "oyosyW7_tj1g",
        "outputId": "0fd6eb22-86b7-4067-9271-ca8cb7806c17"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>account_value</th>\n",
              "      <th>date</th>\n",
              "      <th>daily_return</th>\n",
              "      <th>datadate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>2020-07-02</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-07-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.001318e+06</td>\n",
              "      <td>2020-07-06</td>\n",
              "      <td>0.001318</td>\n",
              "      <td>2020-07-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.992043e+05</td>\n",
              "      <td>2020-07-07</td>\n",
              "      <td>-0.002111</td>\n",
              "      <td>2020-07-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9.994082e+05</td>\n",
              "      <td>2020-07-08</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>2020-07-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9.958046e+05</td>\n",
              "      <td>2020-07-09</td>\n",
              "      <td>-0.003606</td>\n",
              "      <td>2020-07-09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   account_value        date  daily_return    datadate\n",
              "0   1.000000e+06  2020-07-02           NaN  2020-07-02\n",
              "1   1.001318e+06  2020-07-06      0.001318  2020-07-06\n",
              "2   9.992043e+05  2020-07-07     -0.002111  2020-07-07\n",
              "3   9.994082e+05  2020-07-08      0.000204  2020-07-08\n",
              "4   9.958046e+05  2020-07-09     -0.003606  2020-07-09"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_account_value.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "wLsRdw2Ctj1h",
        "outputId": "0837a717-6007-4e61-80a2-319273a56b9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "df_account_value.account_value.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr2zX7ZxNyFQ"
      },
      "source": [
        "<a id='6.1'></a>\n",
        "## 7.1 BackTestStats\n",
        "pass in df_account_value, this information is stored in env class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzkr9yv-AdV_",
        "outputId": "3cdac2b4-999f-4add-9ea7-62e40401b51b",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==============Get Backtest Results===========\n",
            "Annual return          0.402915\n",
            "Cumulative returns     0.402915\n",
            "Annual volatility      0.137987\n",
            "Sharpe ratio           2.533689\n",
            "Calmar ratio           5.360078\n",
            "Stability              0.965695\n",
            "Max drawdown          -0.075170\n",
            "Omega ratio            1.541072\n",
            "Sortino ratio          3.989807\n",
            "Skew                        NaN\n",
            "Kurtosis                    NaN\n",
            "Tail ratio             1.184223\n",
            "Daily value at risk   -0.015997\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print(\"==============Get Backtest Results===========\")\n",
        "now = datetime.now().strftime('%Y%m%d-%Hh%M')\n",
        "\n",
        "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
        "perf_stats_all = pd.DataFrame(perf_stats_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiHhM1YkoCel",
        "outputId": "98980249-804a-40c2-8469-eee53ed6345d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==============Get Baseline Stats===========\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-24:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/wanyao/anaconda3/envs/py372/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/home/wanyao/anaconda3/envs/py372/lib/python3.7/threading.py\", line 865, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/home/wanyao/anaconda3/envs/py372/lib/python3.7/site-packages/multitasking/__init__.py\", line 102, in _run_via_pool\n",
            "    return callee(*args, **kwargs)\n",
            "  File \"/home/wanyao/anaconda3/envs/py372/lib/python3.7/site-packages/yfinance/multi.py\", line 170, in _download_one_threaded\n",
            "    actions, period, interval, prepost, proxy, rounding)\n",
            "  File \"/home/wanyao/anaconda3/envs/py372/lib/python3.7/site-packages/yfinance/multi.py\", line 185, in _download_one\n",
            "    rounding=rounding, many=True)\n",
            "  File \"/home/wanyao/anaconda3/envs/py372/lib/python3.7/site-packages/yfinance/base.py\", line 162, in history\n",
            "    data = data.json()\n",
            "  File \"/home/wanyao/anaconda3/envs/py372/lib/python3.7/site-packages/requests/models.py\", line 900, in json\n",
            "    return complexjson.loads(self.text, **kwargs)\n",
            "  File \"/home/wanyao/anaconda3/envs/py372/lib/python3.7/json/__init__.py\", line 348, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "  File \"/home/wanyao/anaconda3/envs/py372/lib/python3.7/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "  File \"/home/wanyao/anaconda3/envs/py372/lib/python3.7/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#baseline stats\n",
        "print(\"==============Get Baseline Stats===========\")\n",
        "baseline_df = get_baseline(\n",
        "        ticker=\"^DJI\", \n",
        "        start = df_account_value.loc[0,'date'],\n",
        "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
        "\n",
        "stats = backtest_stats(baseline_df, value_col_name = 'close')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U6Suru3h1jc"
      },
      "source": [
        "<a id='6.2'></a>\n",
        "## 7.2 BackTestPlot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HggausPRoCem",
        "outputId": "2d6febda-99b5-472c-abc6-e779585bcc6f"
      },
      "outputs": [],
      "source": [
        "print(\"==============Compare to DJIA===========\")\n",
        "%matplotlib inline\n",
        "# S&P 500: ^GSPC\n",
        "# Dow Jones Index: ^DJI\n",
        "# NASDAQ 100: ^NDX\n",
        "backtest_plot(df_account_value, \n",
        "             baseline_ticker = '^DJI', \n",
        "             baseline_start = df_account_value.loc[0,'date'],\n",
        "             baseline_end = df_account_value.loc[len(df_account_value)-1,'date'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "FinRL_ensemble_stock_trading_ICAIF_2020.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "29b3a9bc54c87c02749c4ff8e4cd4e137b04e73ed6bc96a9e2febe1b1c587ddd"
    },
    "kernelspec": {
      "display_name": "Python 3.7.2 64-bit ('py372': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
