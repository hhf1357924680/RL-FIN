{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maze\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BLOCKING_MAZE1 = ['############',\n",
    "                  '#          #',\n",
    "                  '#          #',\n",
    "                  '#          #',\n",
    "                  '########## #',\n",
    "                  '#          #',\n",
    "                  '#   P      #',\n",
    "                  '############']\n",
    "BLOCKING_MAZE2 = ['############',\n",
    "                  '#          #',\n",
    "                  '#          #',\n",
    "                  '#          #',\n",
    "                  '# ##########',\n",
    "                  '#          #',\n",
    "                  '#   P      #',\n",
    "                  '############']\n",
    "class DynaQ():\n",
    "  def __init__(self, game, n,alpha,gamma, epsilon, max_steps):\n",
    "    self.game = game\n",
    "    self.env = game.make(BLOCKING_MAZE1)\n",
    "    self.q = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "    self.epsilon = epsilon\n",
    "    self.model =  Model(self.env.observation_space.n, self.env.action_space.n)\n",
    "    self.n = n\n",
    "\n",
    "  def learn(self):\n",
    "    \"\"\" Perform DynaQ learning, return cumulative return \"\"\"\n",
    "    s = self.env.reset() # initialize first state\n",
    "    cum_reward = [0] # cumulative reward\n",
    "\n",
    "    # Loop forever!\n",
    "    for step in range(max_steps):\n",
    "      # Epsilon greedy action\n",
    "      if np.random.uniform() < self.epsilon:\n",
    "        a = self.env.action_space.sample()\n",
    "      else:\n",
    "        a = np.random.choice(np.where(self.q[s] == np.max(self.q[s]))[0])\n",
    "\n",
    "      # Take action, observe outcome\n",
    "      s_prime, r, done, info = self.env.step(a)\n",
    "\n",
    "      # Q-Learning\n",
    "      self.q[s,a] += alpha*(r + gamma*np.max(self.q[s_prime]) - self.q[s,a])\n",
    "\n",
    "      # Learn model\n",
    "      self.model.add(s,a,s_prime,r)\n",
    "\n",
    "      # Planning for n steps\n",
    "      self.planning()\n",
    "\n",
    "      # Set state for next loop\n",
    "      s = s_prime\n",
    "\n",
    "      # Reset game if at the end\n",
    "      if done:\n",
    "        s = self.env.reset()\n",
    "\n",
    "      # Check if time to switch board\n",
    "      if step == 1000:\n",
    "        self.env = self.game.make(BLOCKING_MAZE2)\n",
    "        s = self.env.reset()\n",
    "\n",
    "      # Add reward to count\n",
    "      cum_reward.append(cum_reward[-1] + r)\n",
    "\n",
    "    return np.array(cum_reward[1:])\n",
    "\n",
    "  def planning(self):\n",
    "    for i in range(self.n):\n",
    "      s, a =  self.model.sample()\n",
    "      s_prime, r = self.model.step(s,a)\n",
    "      self.q[s,a] += alpha*(r + gamma*np.max(self.q[s_prime]) - self.q[s,a])\n",
    "\n",
    "class Model():\n",
    "  def __init__(self, n_states, n_actions):\n",
    "    self.transitions = np.zeros((n_states,n_actions), dtype=np.uint8)\n",
    "    self.rewards = np.zeros((n_states, n_actions))\n",
    "\n",
    "  def add(self,s,a,s_prime,r):\n",
    "    self.transitions[s,a] = s_prime\n",
    "    self.rewards[s,a] = r\n",
    "\n",
    "  def sample(self):\n",
    "    \"\"\" Return random state, action\"\"\"\n",
    "    # Random visited state\n",
    "    s = np.random.choice(np.where(np.sum(self.transitions, axis=1) > 0)[0])\n",
    "    # Random action in that state\n",
    "    a = np.random.choice(np.where(self.transitions[s] > 0)[0])\n",
    "\n",
    "    return s,a\n",
    "\n",
    "  def step(self, s,a):\n",
    "    \"\"\" Return state_prime and reward for state-action pair\"\"\"\n",
    "    s_prime = self.transitions[s,a]\n",
    "    r = self.rewards[s,a]\n",
    "    return s_prime, r\n",
    "\n",
    "\n",
    "def plot_data(y):\n",
    "  \"\"\" y is a 1D vector \"\"\"\n",
    "  x = np.arange(y.size)\n",
    "  _ = plt.plot(x, y, '-')\n",
    "  plt.show()\n",
    "\n",
    "def multi_plot_data(data, names):\n",
    "  \"\"\" data, names are lists of vectors \"\"\"\n",
    "  x = np.arange(data[0].size)\n",
    "  for i, y in enumerate(data):\n",
    "    plt.plot(x, y, '-', markersize=2, label=names[i])\n",
    "  plt.legend(loc='lower right', prop={'size': 16}, numpoints=5)\n",
    "  plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # Hyperparams\n",
    "\n",
    "  alpha = 0.1 # learning rate\n",
    "  gamma = 0.95 # discount\n",
    "  epsilon = 0.3\n",
    "  max_steps = 3000\n",
    "  trials = 1\n",
    "\n",
    "  dynaq_5_r = np.zeros((trials, max_steps))\n",
    "  dynaq_50_r = np.zeros((trials, max_steps))\n",
    "  qlearning_r = np.zeros((trials, max_steps))\n",
    "  for t in range(trials):\n",
    "    # DynaQ 5\n",
    "    n = 5\n",
    "    agent = DynaQ(maze, n, alpha, gamma, epsilon, max_steps)\n",
    "    dynaq_5_r[t] = agent.learn()\n",
    "\n",
    "    # DynaQ 50\n",
    "    n = 50\n",
    "    agent = DynaQ(maze, n, alpha, gamma, epsilon, max_steps)\n",
    "    dynaq_50_r[t] = agent.learn()\n",
    "\n",
    "    # Q-Learning\n",
    "    n = 0\n",
    "    agent = DynaQ(maze, n, alpha, gamma, epsilon, max_steps)\n",
    "    qlearning_r[t] = agent.learn()\n",
    "\n",
    "  # Average across trials\n",
    "  dynaq_5_r = np.mean(dynaq_5_r, axis=0)\n",
    "  dynaq_50_r = np.mean(dynaq_50_r, axis=0)\n",
    "  qlearning_r = np.mean(qlearning_r, axis=0)\n",
    "\n",
    "  data=[dynaq_5_r, dynaq_50_r, qlearning_r]\n",
    "  names=[\"DynaQ, n=5\", \"DynaQ, n=50\", \"Q-Learning\"]\n",
    "  multi_plot_data(data,names)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fd1e5f8b184b35e6e0916f3a81d0349f025caf8f2595ce6a607932abb53c1f4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('gymenv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
